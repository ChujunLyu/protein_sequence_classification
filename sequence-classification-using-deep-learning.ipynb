{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"We are going to use Deep learning for classify Protein sequnces that they are HBPs or NON-HBPS. \nAlgorithm Used\n\n* Convolutional Neural Network 1d with Embiding Layer\n* Convolutional Neural Network 2d\n* Siamese Neural Network\n* Some Machine Learning Algorithms without Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"# Protein Sequence Reading","metadata":{}},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T10:31:30.001048Z","iopub.execute_input":"2021-12-02T10:31:30.001339Z","iopub.status.idle":"2021-12-02T10:31:30.009808Z","shell.execute_reply.started":"2021-12-02T10:31:30.001289Z","shell.execute_reply":"2021-12-02T10:31:30.008744Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"1.13.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom nltk.tokenize import sent_tokenize, word_tokenize \nimport warnings  \nwarnings.filterwarnings(action = 'ignore')  \nimport gensim \nfrom gensim.models import Word2Vec ","metadata":{"execution":{"iopub.status.busy":"2021-12-02T10:31:32.160538Z","iopub.execute_input":"2021-12-02T10:31:32.160829Z","iopub.status.idle":"2021-12-02T10:31:33.941815Z","shell.execute_reply.started":"2021-12-02T10:31:32.160780Z","shell.execute_reply":"2021-12-02T10:31:33.940740Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_data1 = pd.read_csv('../input/dataset/solubility_train.csv')\ndev_data1 = pd.read_csv('../input/dataset/solubility_dev.csv')\ntest_data = pd.read_csv('../input/dataset/solubility_test.csv')\ntrain_data = pd.concat([train_data1,dev_data1])\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T10:31:35.236392Z","iopub.execute_input":"2021-12-02T10:31:35.236715Z","iopub.status.idle":"2021-12-02T10:31:36.622997Z","shell.execute_reply.started":"2021-12-02T10:31:35.236663Z","shell.execute_reply":"2021-12-02T10:31:36.622338Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"           Meta                                           Sequence  Label\n0  Psol_train_0  GMILKTNLFGHTYQFKSITDVLAKANEEKSGDRLAGVAAESAEERV...      1\n1  Psol_train_1  MAHHHHHHMSFFRMKRRLNFVVKRGIEELWENSFLDNNVDMKKIEY...      0\n2  Psol_train_2  MGSDKIHHHHHHMEKSIQDTIHGVIKLEDWMVEIVDTPQFQRLRRI...      0\n3  Psol_train_3  MEKYIHSVEDYHRLISYLENNLNYEDSVVNHVIYVIAKTGMRYGEI...      0\n4  Psol_train_4  MSLTDSFTVRSIEGVCFRYPLATPVVTSFGKMLNRPAVFVRVVDED...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Meta</th>\n      <th>Sequence</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Psol_train_0</td>\n      <td>GMILKTNLFGHTYQFKSITDVLAKANEEKSGDRLAGVAAESAEERV...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Psol_train_1</td>\n      <td>MAHHHHHHMSFFRMKRRLNFVVKRGIEELWENSFLDNNVDMKKIEY...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Psol_train_2</td>\n      <td>MGSDKIHHHHHHMEKSIQDTIHGVIKLEDWMVEIVDTPQFQRLRRI...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Psol_train_3</td>\n      <td>MEKYIHSVEDYHRLISYLENNLNYEDSVVNHVIYVIAKTGMRYGEI...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Psol_train_4</td>\n      <td>MSLTDSFTVRSIEGVCFRYPLATPVVTSFGKMLNRPAVFVRVVDED...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\nimport keras\n# Transform labels to one-hot\nlb = LabelBinarizer()\ny_train = lb.fit_transform(train_data.Label)\ny_train=keras.utils.to_categorical(y_train,num_classes=2,dtype='float32')\ny_test = lb.fit_transform(test_data.Label)\ny_test=keras.utils.to_categorical(y_test,num_classes=2,dtype='float32')","metadata":{"execution":{"iopub.status.busy":"2021-12-02T10:31:38.563333Z","iopub.execute_input":"2021-12-02T10:31:38.563662Z","iopub.status.idle":"2021-12-02T10:31:38.583820Z","shell.execute_reply.started":"2021-12-02T10:31:38.563608Z","shell.execute_reply":"2021-12-02T10:31:38.582976Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer\nUsing the ** keras** library for text processing, \n1. ** Tokenizer**: translates every character of the sequence into a number\n2. **pad_sequences:** ensures that every sequence has the same length (max_length). I decided to use a maximum length of 100, which should be sufficient for most sequences. \n3. **train_test_split:** from sklearn splits the data into training and testing samples.","metadata":{}},{"cell_type":"code","source":"arr=[]\nfor i in train_data.Sequence:\n    arr.append(len(i))\n    \narr=np.asarray(arr)\nprint(\"Average length of string is = \",(arr.mean()))\nprint(\"Median length of string is = \",np.median(arr))","metadata":{"execution":{"iopub.status.busy":"2021-12-02T10:31:44.498636Z","iopub.execute_input":"2021-12-02T10:31:44.498936Z","iopub.status.idle":"2021-12-02T10:31:44.555270Z","shell.execute_reply.started":"2021-12-02T10:31:44.498885Z","shell.execute_reply":"2021-12-02T10:31:44.554499Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Average length of string is =  298.93048112935753\nMedian length of string is =  275.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split\n\n# maximum length of sequence, everything afterwards is discarded!\nmax_length = 280\n\n#create and fit tokenizer\ntokenizer = Tokenizer(char_level=True)\ntokenizer.fit_on_texts(train_data.Sequence)\n#represent input data as word rank number sequences\nx_train = tokenizer.texts_to_sequences(train_data.Sequence)\nx_train = sequence.pad_sequences(x_train, maxlen=max_length)\nx_test = tokenizer.texts_to_sequences(test_data.Sequence)\nx_test = sequence.pad_sequences(x_test, maxlen=max_length)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T10:31:47.355292Z","iopub.execute_input":"2021-12-02T10:31:47.355593Z","iopub.status.idle":"2021-12-02T10:32:03.700790Z","shell.execute_reply.started":"2021-12-02T10:31:47.355545Z","shell.execute_reply":"2021-12-02T10:32:03.699917Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Conv1D Training ","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten,Dropout,Conv2D\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\n\nembedding_dim = 64\n\n# create the model\nmodel = Sequential()\nmodel.add(Embedding(len(tokenizer.word_index)+1, embedding_dim, input_length=max_length))\nmodel.add(Conv1D(filters=256, kernel_size=2, padding='same', activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv1D(filters=256, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=128, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=16, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\n\nmodel.add(Flatten())\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=200, batch_size=512)\n#embedding_dim = 64ï¼Œepochs=200, batch_size=512","metadata":{"execution":{"iopub.status.busy":"2021-12-02T10:34:49.366361Z","iopub.execute_input":"2021-12-02T10:34:49.366687Z","iopub.status.idle":"2021-12-02T11:14:10.936193Z","shell.execute_reply.started":"2021-12-02T10:34:49.366635Z","shell.execute_reply":"2021-12-02T11:14:10.935469Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 280, 64)           1344      \n_________________________________________________________________\nconv1d_3 (Conv1D)            (None, 280, 256)          33024     \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 280, 256)          0         \n_________________________________________________________________\nconv1d_4 (Conv1D)            (None, 280, 256)          131328    \n_________________________________________________________________\nmax_pooling1d_2 (MaxPooling1 (None, 280, 256)          0         \n_________________________________________________________________\nconv1d_5 (Conv1D)            (None, 280, 128)          65664     \n_________________________________________________________________\nmax_pooling1d_3 (MaxPooling1 (None, 280, 128)          0         \n_________________________________________________________________\nconv1d_6 (Conv1D)            (None, 280, 64)           16448     \n_________________________________________________________________\nmax_pooling1d_4 (MaxPooling1 (None, 280, 64)           0         \n_________________________________________________________________\nconv1d_7 (Conv1D)            (None, 280, 32)           4128      \n_________________________________________________________________\nmax_pooling1d_5 (MaxPooling1 (None, 280, 32)           0         \n_________________________________________________________________\nconv1d_8 (Conv1D)            (None, 280, 16)           1040      \n_________________________________________________________________\nmax_pooling1d_6 (MaxPooling1 (None, 280, 16)           0         \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 4480)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 8)                 35848     \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 8)                 0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 2)                 18        \n=================================================================\nTotal params: 288,842\nTrainable params: 288,842\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 69420 samples, validate on 2001 samples\nEpoch 1/200\n69420/69420 [==============================] - 13s 194us/step - loss: 0.6751 - acc: 0.5828 - val_loss: 0.6828 - val_acc: 0.5637\nEpoch 2/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.6530 - acc: 0.6071 - val_loss: 0.6589 - val_acc: 0.6037\nEpoch 3/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.6356 - acc: 0.6138 - val_loss: 0.6366 - val_acc: 0.6172\nEpoch 4/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.6344 - acc: 0.6125 - val_loss: 0.6403 - val_acc: 0.5847\nEpoch 5/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.6290 - acc: 0.6149 - val_loss: 0.6432 - val_acc: 0.6052\nEpoch 6/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.6264 - acc: 0.6195 - val_loss: 0.6447 - val_acc: 0.5977\nEpoch 7/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.6258 - acc: 0.6191 - val_loss: 0.6482 - val_acc: 0.5942\nEpoch 8/200\n69420/69420 [==============================] - 12s 172us/step - loss: 0.6245 - acc: 0.6201 - val_loss: 0.6311 - val_acc: 0.6122\nEpoch 9/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.6218 - acc: 0.6211 - val_loss: 0.6399 - val_acc: 0.5987\nEpoch 10/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.6190 - acc: 0.6240 - val_loss: 0.6291 - val_acc: 0.6137\nEpoch 11/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.6153 - acc: 0.6270 - val_loss: 0.6418 - val_acc: 0.6162\nEpoch 12/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.6144 - acc: 0.6279 - val_loss: 0.6312 - val_acc: 0.6192\nEpoch 13/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.6112 - acc: 0.6287 - val_loss: 0.6422 - val_acc: 0.6162\nEpoch 14/200\n69420/69420 [==============================] - 12s 172us/step - loss: 0.6063 - acc: 0.6322 - val_loss: 0.6461 - val_acc: 0.6242\nEpoch 15/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.6046 - acc: 0.6520 - val_loss: 0.6336 - val_acc: 0.6732\nEpoch 16/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.6048 - acc: 0.6606 - val_loss: 0.6337 - val_acc: 0.6672\nEpoch 17/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.5982 - acc: 0.6649 - val_loss: 0.6345 - val_acc: 0.6592\nEpoch 18/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.5917 - acc: 0.6629 - val_loss: 0.6260 - val_acc: 0.6347\nEpoch 19/200\n69420/69420 [==============================] - 12s 172us/step - loss: 0.5901 - acc: 0.6587 - val_loss: 0.6185 - val_acc: 0.6382\nEpoch 20/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5839 - acc: 0.6621 - val_loss: 0.6553 - val_acc: 0.6357\nEpoch 21/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5834 - acc: 0.6629 - val_loss: 0.6359 - val_acc: 0.6357\nEpoch 22/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.5784 - acc: 0.6662 - val_loss: 0.6374 - val_acc: 0.6422\nEpoch 23/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5774 - acc: 0.6662 - val_loss: 0.6732 - val_acc: 0.6247\nEpoch 24/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5723 - acc: 0.6690 - val_loss: 0.6309 - val_acc: 0.6347\nEpoch 25/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.5713 - acc: 0.6708 - val_loss: 0.6234 - val_acc: 0.6337\nEpoch 26/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5700 - acc: 0.6703 - val_loss: 0.6394 - val_acc: 0.6342\nEpoch 27/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5660 - acc: 0.6727 - val_loss: 0.6513 - val_acc: 0.6267\nEpoch 28/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.5638 - acc: 0.6731 - val_loss: 0.6325 - val_acc: 0.6387\nEpoch 29/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5621 - acc: 0.6740 - val_loss: 0.6448 - val_acc: 0.6257\nEpoch 30/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.5601 - acc: 0.6745 - val_loss: 0.6558 - val_acc: 0.6307\nEpoch 31/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5587 - acc: 0.6738 - val_loss: 0.6424 - val_acc: 0.6397\nEpoch 32/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5531 - acc: 0.6774 - val_loss: 0.7016 - val_acc: 0.6272\nEpoch 33/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.5513 - acc: 0.6770 - val_loss: 0.6653 - val_acc: 0.6302\nEpoch 34/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5495 - acc: 0.6776 - val_loss: 0.6448 - val_acc: 0.6362\nEpoch 35/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5505 - acc: 0.6756 - val_loss: 0.6578 - val_acc: 0.6382\nEpoch 36/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.5463 - acc: 0.6793 - val_loss: 0.6861 - val_acc: 0.6217\nEpoch 37/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5427 - acc: 0.6787 - val_loss: 0.6612 - val_acc: 0.6402\nEpoch 38/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5401 - acc: 0.6827 - val_loss: 0.6927 - val_acc: 0.6247\nEpoch 39/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.5382 - acc: 0.6828 - val_loss: 0.7217 - val_acc: 0.6207\nEpoch 40/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5375 - acc: 0.6817 - val_loss: 0.7102 - val_acc: 0.6262\nEpoch 41/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5338 - acc: 0.6843 - val_loss: 0.6952 - val_acc: 0.6257\nEpoch 42/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5315 - acc: 0.6866 - val_loss: 0.6966 - val_acc: 0.6287\nEpoch 43/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5308 - acc: 0.6845 - val_loss: 0.7057 - val_acc: 0.6187\nEpoch 44/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.5271 - acc: 0.6847 - val_loss: 0.7270 - val_acc: 0.6132\nEpoch 45/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5254 - acc: 0.6854 - val_loss: 0.7507 - val_acc: 0.6077\nEpoch 46/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5227 - acc: 0.6864 - val_loss: 0.7372 - val_acc: 0.6277\nEpoch 47/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.5198 - acc: 0.6882 - val_loss: 0.7585 - val_acc: 0.6162\nEpoch 48/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5180 - acc: 0.6879 - val_loss: 0.7791 - val_acc: 0.6137\nEpoch 49/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5145 - acc: 0.6908 - val_loss: 0.7662 - val_acc: 0.6107\nEpoch 50/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.5111 - acc: 0.6915 - val_loss: 0.7645 - val_acc: 0.6227\nEpoch 51/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5097 - acc: 0.6922 - val_loss: 0.7544 - val_acc: 0.6212\nEpoch 52/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5102 - acc: 0.6905 - val_loss: 0.7355 - val_acc: 0.6227\nEpoch 53/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5067 - acc: 0.6948 - val_loss: 0.7642 - val_acc: 0.6122\nEpoch 54/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5026 - acc: 0.6963 - val_loss: 0.8058 - val_acc: 0.6182\nEpoch 55/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.5033 - acc: 0.6955 - val_loss: 0.8036 - val_acc: 0.6062\nEpoch 56/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4978 - acc: 0.6963 - val_loss: 0.7906 - val_acc: 0.6127\nEpoch 57/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.5004 - acc: 0.6963 - val_loss: 0.8092 - val_acc: 0.6042\nEpoch 58/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4988 - acc: 0.6966 - val_loss: 0.8681 - val_acc: 0.6047\nEpoch 59/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4928 - acc: 0.6983 - val_loss: 0.8691 - val_acc: 0.6112\nEpoch 60/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4927 - acc: 0.6995 - val_loss: 0.8155 - val_acc: 0.6132\nEpoch 61/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4915 - acc: 0.6983 - val_loss: 0.8340 - val_acc: 0.6187\nEpoch 62/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4896 - acc: 0.6991 - val_loss: 0.8368 - val_acc: 0.6102\nEpoch 63/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4895 - acc: 0.7002 - val_loss: 0.8231 - val_acc: 0.6147\nEpoch 64/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4856 - acc: 0.7018 - val_loss: 0.8544 - val_acc: 0.6067\nEpoch 65/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4868 - acc: 0.7027 - val_loss: 0.8849 - val_acc: 0.6042\nEpoch 66/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4832 - acc: 0.7029 - val_loss: 0.8763 - val_acc: 0.6007\nEpoch 67/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4808 - acc: 0.7023 - val_loss: 0.8827 - val_acc: 0.6062\nEpoch 68/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4828 - acc: 0.7023 - val_loss: 0.9275 - val_acc: 0.6112\nEpoch 69/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4785 - acc: 0.7037 - val_loss: 0.9590 - val_acc: 0.6007\nEpoch 70/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4782 - acc: 0.7040 - val_loss: 0.8829 - val_acc: 0.6162\nEpoch 71/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4763 - acc: 0.7036 - val_loss: 0.9017 - val_acc: 0.6142\nEpoch 72/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4759 - acc: 0.7040 - val_loss: 0.8666 - val_acc: 0.6082\nEpoch 73/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4742 - acc: 0.7049 - val_loss: 0.9027 - val_acc: 0.6027\nEpoch 74/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4724 - acc: 0.7047 - val_loss: 0.9669 - val_acc: 0.6017\nEpoch 75/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4728 - acc: 0.7073 - val_loss: 0.9747 - val_acc: 0.6052\nEpoch 76/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4685 - acc: 0.7077 - val_loss: 0.8941 - val_acc: 0.6102\nEpoch 77/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4720 - acc: 0.7070 - val_loss: 0.8542 - val_acc: 0.6137\nEpoch 78/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4659 - acc: 0.7087 - val_loss: 0.9364 - val_acc: 0.6092\nEpoch 79/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4656 - acc: 0.7093 - val_loss: 0.9872 - val_acc: 0.5987\nEpoch 80/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4636 - acc: 0.7104 - val_loss: 0.9804 - val_acc: 0.6062\nEpoch 81/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4668 - acc: 0.7095 - val_loss: 0.9875 - val_acc: 0.6017\nEpoch 82/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4609 - acc: 0.7104 - val_loss: 0.9771 - val_acc: 0.6172\nEpoch 83/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4595 - acc: 0.7111 - val_loss: 0.9626 - val_acc: 0.6032\nEpoch 84/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4573 - acc: 0.7110 - val_loss: 0.9805 - val_acc: 0.5997\nEpoch 85/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4579 - acc: 0.7119 - val_loss: 0.9945 - val_acc: 0.6072\nEpoch 86/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4583 - acc: 0.7112 - val_loss: 0.9893 - val_acc: 0.6042\nEpoch 87/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4581 - acc: 0.7119 - val_loss: 0.9449 - val_acc: 0.6067\nEpoch 88/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4564 - acc: 0.7137 - val_loss: 0.9048 - val_acc: 0.6022\nEpoch 89/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4553 - acc: 0.7119 - val_loss: 0.9601 - val_acc: 0.6012\nEpoch 90/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4537 - acc: 0.7145 - val_loss: 1.0760 - val_acc: 0.5942\nEpoch 91/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4541 - acc: 0.7143 - val_loss: 0.9713 - val_acc: 0.6107\nEpoch 92/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4510 - acc: 0.7164 - val_loss: 1.0099 - val_acc: 0.5987\nEpoch 93/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4520 - acc: 0.7149 - val_loss: 1.0170 - val_acc: 0.5972\nEpoch 94/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4481 - acc: 0.7212 - val_loss: 1.0088 - val_acc: 0.6367\nEpoch 95/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4486 - acc: 0.7214 - val_loss: 1.0369 - val_acc: 0.6482\nEpoch 96/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4479 - acc: 0.7181 - val_loss: 0.9782 - val_acc: 0.6062\nEpoch 97/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4449 - acc: 0.7171 - val_loss: 1.0450 - val_acc: 0.6017\nEpoch 98/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4442 - acc: 0.7181 - val_loss: 1.0880 - val_acc: 0.6107\nEpoch 99/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4427 - acc: 0.7211 - val_loss: 1.0540 - val_acc: 0.6477\nEpoch 100/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4436 - acc: 0.7183 - val_loss: 1.0268 - val_acc: 0.6017\nEpoch 101/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4438 - acc: 0.7170 - val_loss: 1.0151 - val_acc: 0.6087\nEpoch 102/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4427 - acc: 0.7201 - val_loss: 1.1259 - val_acc: 0.6052\nEpoch 103/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4396 - acc: 0.7229 - val_loss: 1.1766 - val_acc: 0.5967\nEpoch 104/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4403 - acc: 0.7258 - val_loss: 1.1042 - val_acc: 0.6247\nEpoch 105/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4384 - acc: 0.7245 - val_loss: 1.0436 - val_acc: 0.6352\nEpoch 106/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4393 - acc: 0.7274 - val_loss: 1.0886 - val_acc: 0.6377\nEpoch 107/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4414 - acc: 0.7262 - val_loss: 1.1645 - val_acc: 0.5937\nEpoch 108/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4367 - acc: 0.7239 - val_loss: 1.0229 - val_acc: 0.6017\nEpoch 109/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4349 - acc: 0.7273 - val_loss: 1.0227 - val_acc: 0.6312\nEpoch 110/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4364 - acc: 0.7293 - val_loss: 1.0885 - val_acc: 0.6427\nEpoch 111/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4324 - acc: 0.7325 - val_loss: 1.0791 - val_acc: 0.6327\nEpoch 112/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4321 - acc: 0.7308 - val_loss: 1.0899 - val_acc: 0.6337\nEpoch 113/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4355 - acc: 0.7297 - val_loss: 1.1235 - val_acc: 0.6327\nEpoch 114/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4302 - acc: 0.7327 - val_loss: 1.0971 - val_acc: 0.6287\nEpoch 115/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4285 - acc: 0.7315 - val_loss: 1.1288 - val_acc: 0.6222\nEpoch 116/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4279 - acc: 0.7349 - val_loss: 1.1884 - val_acc: 0.6357\nEpoch 117/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4321 - acc: 0.7304 - val_loss: 1.1073 - val_acc: 0.6362\nEpoch 118/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4278 - acc: 0.7335 - val_loss: 1.0900 - val_acc: 0.6297\nEpoch 119/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4276 - acc: 0.7333 - val_loss: 1.1228 - val_acc: 0.6322\nEpoch 120/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4248 - acc: 0.7357 - val_loss: 1.1094 - val_acc: 0.6267\nEpoch 121/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4297 - acc: 0.7350 - val_loss: 1.0930 - val_acc: 0.6322\nEpoch 122/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4253 - acc: 0.7359 - val_loss: 1.0832 - val_acc: 0.6277\nEpoch 123/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4239 - acc: 0.7370 - val_loss: 1.1418 - val_acc: 0.6257\nEpoch 124/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4243 - acc: 0.7364 - val_loss: 1.1882 - val_acc: 0.6347\nEpoch 125/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4215 - acc: 0.7361 - val_loss: 1.1853 - val_acc: 0.6272\nEpoch 126/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4243 - acc: 0.7362 - val_loss: 1.1267 - val_acc: 0.6247\nEpoch 127/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4230 - acc: 0.7373 - val_loss: 1.1123 - val_acc: 0.6312\nEpoch 128/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4190 - acc: 0.7399 - val_loss: 1.2684 - val_acc: 0.6407\nEpoch 129/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4191 - acc: 0.7396 - val_loss: 1.1879 - val_acc: 0.6322\nEpoch 130/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4186 - acc: 0.7395 - val_loss: 1.1496 - val_acc: 0.6307\nEpoch 131/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4196 - acc: 0.7400 - val_loss: 1.2212 - val_acc: 0.6377\nEpoch 132/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4151 - acc: 0.7416 - val_loss: 1.2109 - val_acc: 0.6287\nEpoch 133/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4178 - acc: 0.7393 - val_loss: 1.1885 - val_acc: 0.6357\nEpoch 134/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4191 - acc: 0.7410 - val_loss: 1.1062 - val_acc: 0.6277\nEpoch 135/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4182 - acc: 0.7399 - val_loss: 1.2458 - val_acc: 0.6372\nEpoch 136/200\n69420/69420 [==============================] - 12s 172us/step - loss: 0.4138 - acc: 0.7420 - val_loss: 1.2963 - val_acc: 0.6342\nEpoch 137/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4167 - acc: 0.7411 - val_loss: 1.2123 - val_acc: 0.6447\nEpoch 138/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4169 - acc: 0.7396 - val_loss: 1.2851 - val_acc: 0.6422\nEpoch 139/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4132 - acc: 0.7463 - val_loss: 1.2025 - val_acc: 0.6242\nEpoch 140/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4143 - acc: 0.7447 - val_loss: 1.1697 - val_acc: 0.6202\nEpoch 141/200\n69420/69420 [==============================] - 12s 171us/step - loss: 0.4134 - acc: 0.7434 - val_loss: 1.2591 - val_acc: 0.6277\nEpoch 142/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4118 - acc: 0.7440 - val_loss: 1.2013 - val_acc: 0.6232\nEpoch 143/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.4122 - acc: 0.7456 - val_loss: 1.2831 - val_acc: 0.6357\nEpoch 144/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.4130 - acc: 0.7432 - val_loss: 1.3230 - val_acc: 0.6397\nEpoch 145/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.4120 - acc: 0.7446 - val_loss: 1.2234 - val_acc: 0.6332\nEpoch 146/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.4103 - acc: 0.7457 - val_loss: 1.2939 - val_acc: 0.6332\nEpoch 147/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.4091 - acc: 0.7480 - val_loss: 1.2553 - val_acc: 0.6297\nEpoch 148/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.4095 - acc: 0.7476 - val_loss: 1.2132 - val_acc: 0.6267\nEpoch 149/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.4053 - acc: 0.7490 - val_loss: 1.3085 - val_acc: 0.6417\nEpoch 150/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.4082 - acc: 0.7472 - val_loss: 1.3126 - val_acc: 0.6362\nEpoch 151/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.4045 - acc: 0.7495 - val_loss: 1.2633 - val_acc: 0.6407\nEpoch 152/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.4074 - acc: 0.7483 - val_loss: 1.2978 - val_acc: 0.6232\nEpoch 153/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.4024 - acc: 0.7531 - val_loss: 1.2684 - val_acc: 0.6322\nEpoch 154/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.4052 - acc: 0.7492 - val_loss: 1.2885 - val_acc: 0.6302\nEpoch 155/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.4053 - acc: 0.7493 - val_loss: 1.1880 - val_acc: 0.6317\nEpoch 156/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.4017 - acc: 0.7518 - val_loss: 1.2644 - val_acc: 0.6317\nEpoch 157/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.4003 - acc: 0.7522 - val_loss: 1.2901 - val_acc: 0.6342\nEpoch 158/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.4031 - acc: 0.7517 - val_loss: 1.2555 - val_acc: 0.6197\nEpoch 159/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3992 - acc: 0.7539 - val_loss: 1.2622 - val_acc: 0.6307\nEpoch 160/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.4043 - acc: 0.7506 - val_loss: 1.2096 - val_acc: 0.6267\nEpoch 161/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.4008 - acc: 0.7526 - val_loss: 1.3173 - val_acc: 0.6317\nEpoch 162/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3996 - acc: 0.7535 - val_loss: 1.2629 - val_acc: 0.6327\nEpoch 163/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.4005 - acc: 0.7524 - val_loss: 1.2482 - val_acc: 0.6397\nEpoch 164/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3984 - acc: 0.7560 - val_loss: 1.2629 - val_acc: 0.6412\nEpoch 165/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3981 - acc: 0.7536 - val_loss: 1.3191 - val_acc: 0.6352\nEpoch 166/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3982 - acc: 0.7543 - val_loss: 1.2776 - val_acc: 0.6337\nEpoch 167/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3985 - acc: 0.7559 - val_loss: 1.3117 - val_acc: 0.6282\nEpoch 168/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3985 - acc: 0.7555 - val_loss: 1.3244 - val_acc: 0.6312\nEpoch 169/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.3942 - acc: 0.7561 - val_loss: 1.4160 - val_acc: 0.6347\nEpoch 170/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3969 - acc: 0.7546 - val_loss: 1.3718 - val_acc: 0.6347\nEpoch 171/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3974 - acc: 0.7562 - val_loss: 1.2591 - val_acc: 0.6387\nEpoch 172/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.3968 - acc: 0.7569 - val_loss: 1.3265 - val_acc: 0.6387\nEpoch 173/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3950 - acc: 0.7564 - val_loss: 1.3473 - val_acc: 0.6347\nEpoch 174/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3963 - acc: 0.7561 - val_loss: 1.3744 - val_acc: 0.6317\nEpoch 175/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3937 - acc: 0.7573 - val_loss: 1.4085 - val_acc: 0.6387\nEpoch 176/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3925 - acc: 0.7600 - val_loss: 1.3000 - val_acc: 0.6382\nEpoch 177/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3924 - acc: 0.7586 - val_loss: 1.3615 - val_acc: 0.6307\nEpoch 178/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3915 - acc: 0.7601 - val_loss: 1.4047 - val_acc: 0.6332\nEpoch 179/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3950 - acc: 0.7564 - val_loss: 1.3811 - val_acc: 0.6272\nEpoch 180/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3910 - acc: 0.7587 - val_loss: 1.4111 - val_acc: 0.6217\nEpoch 181/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3932 - acc: 0.7582 - val_loss: 1.3544 - val_acc: 0.6312\nEpoch 182/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3866 - acc: 0.7619 - val_loss: 1.4483 - val_acc: 0.6287\nEpoch 183/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3928 - acc: 0.7594 - val_loss: 1.3264 - val_acc: 0.6322\nEpoch 184/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3874 - acc: 0.7625 - val_loss: 1.3722 - val_acc: 0.6302\nEpoch 185/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3884 - acc: 0.7618 - val_loss: 1.3578 - val_acc: 0.6312\nEpoch 186/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3879 - acc: 0.7622 - val_loss: 1.4105 - val_acc: 0.6377\nEpoch 187/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3907 - acc: 0.7606 - val_loss: 1.3562 - val_acc: 0.6272\nEpoch 188/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3876 - acc: 0.7628 - val_loss: 1.4807 - val_acc: 0.6377\nEpoch 189/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3855 - acc: 0.7638 - val_loss: 1.4041 - val_acc: 0.6317\nEpoch 190/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3853 - acc: 0.7626 - val_loss: 1.4456 - val_acc: 0.6272\nEpoch 191/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3868 - acc: 0.7622 - val_loss: 1.2857 - val_acc: 0.6257\nEpoch 192/200\n69420/69420 [==============================] - 12s 170us/step - loss: 0.3862 - acc: 0.7606 - val_loss: 1.4096 - val_acc: 0.6327\nEpoch 193/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3867 - acc: 0.7649 - val_loss: 1.3711 - val_acc: 0.6332\nEpoch 194/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3830 - acc: 0.7653 - val_loss: 1.3620 - val_acc: 0.6312\nEpoch 195/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3826 - acc: 0.7667 - val_loss: 1.4154 - val_acc: 0.6327\nEpoch 196/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3829 - acc: 0.7662 - val_loss: 1.3704 - val_acc: 0.6197\nEpoch 197/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3819 - acc: 0.7666 - val_loss: 1.4494 - val_acc: 0.6287\nEpoch 198/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3836 - acc: 0.7650 - val_loss: 1.3596 - val_acc: 0.6272\nEpoch 199/200\n69420/69420 [==============================] - 12s 168us/step - loss: 0.3821 - acc: 0.7663 - val_loss: 1.4089 - val_acc: 0.6307\nEpoch 200/200\n69420/69420 [==============================] - 12s 169us/step - loss: 0.3818 - acc: 0.7654 - val_loss: 1.3869 - val_acc: 0.6257\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f6fa0c7d780>"},"metadata":{}}]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten,Dropout,Conv2D\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\n\nembedding_dim = 64\n\n# create the model\nmodel = Sequential()\nmodel.add(Embedding(len(tokenizer.word_index)+1, embedding_dim, input_length=max_length))\nmodel.add(Conv1D(filters=256, kernel_size=2, padding='same', activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv1D(filters=256, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=128, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=16, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\n\nmodel.add(Flatten())\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=200, batch_size=256)\n#embedding_dim = 64ï¼Œepochs=200, batch_size=256","metadata":{"execution":{"iopub.status.busy":"2021-12-02T11:18:00.731779Z","iopub.execute_input":"2021-12-02T11:18:00.732083Z","iopub.status.idle":"2021-12-02T11:59:32.680891Z","shell.execute_reply.started":"2021-12-02T11:18:00.732025Z","shell.execute_reply":"2021-12-02T11:59:32.679944Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (None, 280, 64)           1344      \n_________________________________________________________________\nconv1d_9 (Conv1D)            (None, 280, 256)          33024     \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 280, 256)          0         \n_________________________________________________________________\nconv1d_10 (Conv1D)           (None, 280, 256)          131328    \n_________________________________________________________________\nmax_pooling1d_7 (MaxPooling1 (None, 280, 256)          0         \n_________________________________________________________________\nconv1d_11 (Conv1D)           (None, 280, 128)          65664     \n_________________________________________________________________\nmax_pooling1d_8 (MaxPooling1 (None, 280, 128)          0         \n_________________________________________________________________\nconv1d_12 (Conv1D)           (None, 280, 64)           16448     \n_________________________________________________________________\nmax_pooling1d_9 (MaxPooling1 (None, 280, 64)           0         \n_________________________________________________________________\nconv1d_13 (Conv1D)           (None, 280, 32)           4128      \n_________________________________________________________________\nmax_pooling1d_10 (MaxPooling (None, 280, 32)           0         \n_________________________________________________________________\nconv1d_14 (Conv1D)           (None, 280, 16)           1040      \n_________________________________________________________________\nmax_pooling1d_11 (MaxPooling (None, 280, 16)           0         \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 4480)              0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 8)                 35848     \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 8)                 0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 2)                 18        \n=================================================================\nTotal params: 288,842\nTrainable params: 288,842\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 69420 samples, validate on 2001 samples\nEpoch 1/200\n69420/69420 [==============================] - 14s 196us/step - loss: 0.6631 - acc: 0.5824 - val_loss: 0.6705 - val_acc: 0.5002\nEpoch 2/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.6506 - acc: 0.6025 - val_loss: 0.6621 - val_acc: 0.6467\nEpoch 3/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.6430 - acc: 0.6234 - val_loss: 0.6417 - val_acc: 0.6512\nEpoch 4/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.6387 - acc: 0.6258 - val_loss: 0.6430 - val_acc: 0.6277\nEpoch 5/200\n69420/69420 [==============================] - 13s 181us/step - loss: 0.6360 - acc: 0.6299 - val_loss: 0.6413 - val_acc: 0.6367\nEpoch 6/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.6325 - acc: 0.6329 - val_loss: 0.6357 - val_acc: 0.6432\nEpoch 7/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.6321 - acc: 0.6336 - val_loss: 0.6460 - val_acc: 0.6272\nEpoch 8/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.6295 - acc: 0.6371 - val_loss: 0.6349 - val_acc: 0.6517\nEpoch 9/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.6284 - acc: 0.6383 - val_loss: 0.6406 - val_acc: 0.6237\nEpoch 10/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.6282 - acc: 0.6377 - val_loss: 0.6615 - val_acc: 0.6032\nEpoch 11/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.6232 - acc: 0.6437 - val_loss: 0.6364 - val_acc: 0.6342\nEpoch 12/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.6231 - acc: 0.6420 - val_loss: 0.6515 - val_acc: 0.6407\nEpoch 13/200\n69420/69420 [==============================] - 13s 181us/step - loss: 0.6200 - acc: 0.6478 - val_loss: 0.6451 - val_acc: 0.6277\nEpoch 14/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.6185 - acc: 0.6487 - val_loss: 0.6453 - val_acc: 0.6297\nEpoch 15/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.6170 - acc: 0.6521 - val_loss: 0.6552 - val_acc: 0.6317\nEpoch 16/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.6150 - acc: 0.6516 - val_loss: 0.6503 - val_acc: 0.6232\nEpoch 17/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.6073 - acc: 0.6575 - val_loss: 0.6446 - val_acc: 0.6377\nEpoch 18/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.6052 - acc: 0.6590 - val_loss: 0.6493 - val_acc: 0.6417\nEpoch 19/200\n69420/69420 [==============================] - 12s 178us/step - loss: 0.6011 - acc: 0.6614 - val_loss: 0.6432 - val_acc: 0.6457\nEpoch 20/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5984 - acc: 0.6644 - val_loss: 0.6401 - val_acc: 0.6507\nEpoch 21/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.5923 - acc: 0.6683 - val_loss: 0.6702 - val_acc: 0.6262\nEpoch 22/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5883 - acc: 0.6729 - val_loss: 0.6515 - val_acc: 0.6402\nEpoch 23/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5856 - acc: 0.6770 - val_loss: 0.6493 - val_acc: 0.6482\nEpoch 24/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.5795 - acc: 0.6784 - val_loss: 0.6594 - val_acc: 0.6432\nEpoch 25/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5779 - acc: 0.6776 - val_loss: 0.6684 - val_acc: 0.6397\nEpoch 26/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.5773 - acc: 0.6783 - val_loss: 0.6464 - val_acc: 0.6567\nEpoch 27/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5740 - acc: 0.6807 - val_loss: 0.6678 - val_acc: 0.6527\nEpoch 28/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5699 - acc: 0.6827 - val_loss: 0.6366 - val_acc: 0.6582\nEpoch 29/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.5682 - acc: 0.6822 - val_loss: 0.7047 - val_acc: 0.6542\nEpoch 30/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5654 - acc: 0.6837 - val_loss: 0.6427 - val_acc: 0.6527\nEpoch 31/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5614 - acc: 0.6845 - val_loss: 0.6801 - val_acc: 0.6342\nEpoch 32/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.5611 - acc: 0.6861 - val_loss: 0.6537 - val_acc: 0.6437\nEpoch 33/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5582 - acc: 0.6871 - val_loss: 0.6952 - val_acc: 0.6442\nEpoch 34/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.5554 - acc: 0.6902 - val_loss: 0.7001 - val_acc: 0.6387\nEpoch 35/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5551 - acc: 0.6902 - val_loss: 0.6980 - val_acc: 0.6482\nEpoch 36/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5529 - acc: 0.6909 - val_loss: 0.6754 - val_acc: 0.6502\nEpoch 37/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.5501 - acc: 0.6926 - val_loss: 0.6940 - val_acc: 0.6407\nEpoch 38/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5476 - acc: 0.6939 - val_loss: 0.6629 - val_acc: 0.6307\nEpoch 39/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5449 - acc: 0.6939 - val_loss: 0.7021 - val_acc: 0.6552\nEpoch 40/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.5445 - acc: 0.6929 - val_loss: 0.7082 - val_acc: 0.6582\nEpoch 41/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5443 - acc: 0.6931 - val_loss: 0.6957 - val_acc: 0.6462\nEpoch 42/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.5401 - acc: 0.6965 - val_loss: 0.7209 - val_acc: 0.6432\nEpoch 43/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5384 - acc: 0.6966 - val_loss: 0.7044 - val_acc: 0.6437\nEpoch 44/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5378 - acc: 0.6966 - val_loss: 0.7514 - val_acc: 0.6462\nEpoch 45/200\n69420/69420 [==============================] - 13s 181us/step - loss: 0.5345 - acc: 0.6966 - val_loss: 0.7212 - val_acc: 0.6472\nEpoch 46/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5343 - acc: 0.6972 - val_loss: 0.7655 - val_acc: 0.6432\nEpoch 47/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.5324 - acc: 0.6963 - val_loss: 0.7119 - val_acc: 0.6387\nEpoch 48/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5301 - acc: 0.6999 - val_loss: 0.7726 - val_acc: 0.6422\nEpoch 49/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5291 - acc: 0.6998 - val_loss: 0.7261 - val_acc: 0.6452\nEpoch 50/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.5247 - acc: 0.7038 - val_loss: 0.8567 - val_acc: 0.6402\nEpoch 51/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5244 - acc: 0.7028 - val_loss: 0.7483 - val_acc: 0.6357\nEpoch 52/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5244 - acc: 0.7005 - val_loss: 0.7825 - val_acc: 0.6432\nEpoch 53/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.5228 - acc: 0.7010 - val_loss: 0.7638 - val_acc: 0.6342\nEpoch 54/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5206 - acc: 0.7050 - val_loss: 0.7795 - val_acc: 0.6277\nEpoch 55/200\n69420/69420 [==============================] - 13s 181us/step - loss: 0.5183 - acc: 0.7049 - val_loss: 0.7906 - val_acc: 0.6277\nEpoch 56/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5191 - acc: 0.7048 - val_loss: 0.8740 - val_acc: 0.6377\nEpoch 57/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5161 - acc: 0.7073 - val_loss: 0.7547 - val_acc: 0.6387\nEpoch 58/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.5131 - acc: 0.7060 - val_loss: 0.7886 - val_acc: 0.6362\nEpoch 59/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5123 - acc: 0.7071 - val_loss: 0.8057 - val_acc: 0.6432\nEpoch 60/200\n69420/69420 [==============================] - 13s 181us/step - loss: 0.5104 - acc: 0.7090 - val_loss: 0.8259 - val_acc: 0.6337\nEpoch 61/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5109 - acc: 0.7088 - val_loss: 0.8578 - val_acc: 0.6312\nEpoch 62/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5094 - acc: 0.7102 - val_loss: 0.7891 - val_acc: 0.6392\nEpoch 63/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.5063 - acc: 0.7117 - val_loss: 0.8207 - val_acc: 0.6417\nEpoch 64/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5043 - acc: 0.7141 - val_loss: 0.8617 - val_acc: 0.6377\nEpoch 65/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.5042 - acc: 0.7127 - val_loss: 0.9024 - val_acc: 0.6322\nEpoch 66/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5030 - acc: 0.7142 - val_loss: 0.8875 - val_acc: 0.6347\nEpoch 67/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.5007 - acc: 0.7154 - val_loss: 0.7938 - val_acc: 0.6282\nEpoch 68/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.5000 - acc: 0.7148 - val_loss: 0.9013 - val_acc: 0.6417\nEpoch 69/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4991 - acc: 0.7150 - val_loss: 0.8951 - val_acc: 0.6352\nEpoch 70/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4958 - acc: 0.7163 - val_loss: 0.8494 - val_acc: 0.6332\nEpoch 71/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4910 - acc: 0.7181 - val_loss: 0.8575 - val_acc: 0.6282\nEpoch 72/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4910 - acc: 0.7160 - val_loss: 0.9473 - val_acc: 0.6412\nEpoch 73/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4914 - acc: 0.7156 - val_loss: 0.9399 - val_acc: 0.6402\nEpoch 74/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4883 - acc: 0.7164 - val_loss: 0.9033 - val_acc: 0.6332\nEpoch 75/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4861 - acc: 0.7197 - val_loss: 0.9369 - val_acc: 0.6422\nEpoch 76/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4822 - acc: 0.7211 - val_loss: 0.9296 - val_acc: 0.6297\nEpoch 77/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4820 - acc: 0.7210 - val_loss: 0.9332 - val_acc: 0.6327\nEpoch 78/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4820 - acc: 0.7219 - val_loss: 0.8728 - val_acc: 0.6357\nEpoch 79/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4805 - acc: 0.7210 - val_loss: 0.8771 - val_acc: 0.6462\nEpoch 80/200\n69420/69420 [==============================] - 12s 178us/step - loss: 0.4771 - acc: 0.7241 - val_loss: 0.8968 - val_acc: 0.6292\nEpoch 81/200\n69420/69420 [==============================] - 13s 181us/step - loss: 0.4761 - acc: 0.7230 - val_loss: 0.9268 - val_acc: 0.6312\nEpoch 82/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4769 - acc: 0.7228 - val_loss: 0.9440 - val_acc: 0.6377\nEpoch 83/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4764 - acc: 0.7224 - val_loss: 0.8968 - val_acc: 0.6347\nEpoch 84/200\n69420/69420 [==============================] - 13s 181us/step - loss: 0.4749 - acc: 0.7210 - val_loss: 0.8958 - val_acc: 0.6337\nEpoch 85/200\n69420/69420 [==============================] - 12s 178us/step - loss: 0.4736 - acc: 0.7238 - val_loss: 0.8619 - val_acc: 0.6317\nEpoch 86/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4692 - acc: 0.7260 - val_loss: 0.9552 - val_acc: 0.6232\nEpoch 87/200\n69420/69420 [==============================] - 13s 181us/step - loss: 0.4708 - acc: 0.7248 - val_loss: 0.9858 - val_acc: 0.6337\nEpoch 88/200\n69420/69420 [==============================] - 12s 178us/step - loss: 0.4720 - acc: 0.7264 - val_loss: 0.9418 - val_acc: 0.6207\nEpoch 89/200\n69420/69420 [==============================] - 13s 181us/step - loss: 0.4702 - acc: 0.7266 - val_loss: 1.0007 - val_acc: 0.6337\nEpoch 90/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4666 - acc: 0.7282 - val_loss: 0.9514 - val_acc: 0.6302\nEpoch 91/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4656 - acc: 0.7288 - val_loss: 1.0172 - val_acc: 0.6297\nEpoch 92/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4647 - acc: 0.7308 - val_loss: 1.0464 - val_acc: 0.6342\nEpoch 93/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4639 - acc: 0.7288 - val_loss: 0.9989 - val_acc: 0.6372\nEpoch 94/200\n69420/69420 [==============================] - 12s 178us/step - loss: 0.4645 - acc: 0.7285 - val_loss: 0.9733 - val_acc: 0.6202\nEpoch 95/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4614 - acc: 0.7315 - val_loss: 0.9759 - val_acc: 0.6357\nEpoch 96/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4584 - acc: 0.7311 - val_loss: 1.0389 - val_acc: 0.6457\nEpoch 97/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4588 - acc: 0.7321 - val_loss: 1.1070 - val_acc: 0.6292\nEpoch 98/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4596 - acc: 0.7315 - val_loss: 1.0308 - val_acc: 0.6352\nEpoch 99/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4563 - acc: 0.7342 - val_loss: 1.0405 - val_acc: 0.6342\nEpoch 100/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4564 - acc: 0.7331 - val_loss: 0.9695 - val_acc: 0.6287\nEpoch 101/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4558 - acc: 0.7350 - val_loss: 0.9610 - val_acc: 0.6297\nEpoch 102/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4545 - acc: 0.7346 - val_loss: 1.0494 - val_acc: 0.6367\nEpoch 103/200\n69420/69420 [==============================] - 13s 181us/step - loss: 0.4542 - acc: 0.7356 - val_loss: 1.0953 - val_acc: 0.6357\nEpoch 104/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4519 - acc: 0.7366 - val_loss: 1.0724 - val_acc: 0.6327\nEpoch 105/200\n69420/69420 [==============================] - 13s 181us/step - loss: 0.4531 - acc: 0.7361 - val_loss: 1.0225 - val_acc: 0.6457\nEpoch 106/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4521 - acc: 0.7364 - val_loss: 1.0237 - val_acc: 0.6372\nEpoch 107/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4498 - acc: 0.7381 - val_loss: 1.1159 - val_acc: 0.6347\nEpoch 108/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4516 - acc: 0.7370 - val_loss: 1.0827 - val_acc: 0.6347\nEpoch 109/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4472 - acc: 0.7378 - val_loss: 1.0543 - val_acc: 0.6317\nEpoch 110/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4473 - acc: 0.7391 - val_loss: 1.1652 - val_acc: 0.6257\nEpoch 111/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4493 - acc: 0.7379 - val_loss: 1.0769 - val_acc: 0.6312\nEpoch 112/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4471 - acc: 0.7406 - val_loss: 1.1541 - val_acc: 0.6322\nEpoch 113/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4449 - acc: 0.7410 - val_loss: 1.1375 - val_acc: 0.6327\nEpoch 114/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4422 - acc: 0.7431 - val_loss: 1.0967 - val_acc: 0.6257\nEpoch 115/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4450 - acc: 0.7406 - val_loss: 1.0405 - val_acc: 0.6242\nEpoch 116/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4448 - acc: 0.7416 - val_loss: 1.1944 - val_acc: 0.6367\nEpoch 117/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4428 - acc: 0.7449 - val_loss: 1.0930 - val_acc: 0.6327\nEpoch 118/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4432 - acc: 0.7432 - val_loss: 1.1242 - val_acc: 0.6202\nEpoch 119/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4412 - acc: 0.7451 - val_loss: 1.0823 - val_acc: 0.6247\nEpoch 120/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4383 - acc: 0.7454 - val_loss: 1.1854 - val_acc: 0.6327\nEpoch 121/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4393 - acc: 0.7456 - val_loss: 1.1097 - val_acc: 0.6287\nEpoch 122/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4371 - acc: 0.7462 - val_loss: 1.1708 - val_acc: 0.6282\nEpoch 123/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4396 - acc: 0.7451 - val_loss: 1.1683 - val_acc: 0.6282\nEpoch 124/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4373 - acc: 0.7447 - val_loss: 1.0424 - val_acc: 0.6242\nEpoch 125/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4376 - acc: 0.7450 - val_loss: 1.0684 - val_acc: 0.6257\nEpoch 126/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4375 - acc: 0.7452 - val_loss: 1.2168 - val_acc: 0.6317\nEpoch 127/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4351 - acc: 0.7480 - val_loss: 1.1673 - val_acc: 0.6342\nEpoch 128/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4361 - acc: 0.7458 - val_loss: 1.1960 - val_acc: 0.6382\nEpoch 129/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4357 - acc: 0.7470 - val_loss: 1.0957 - val_acc: 0.6302\nEpoch 130/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4331 - acc: 0.7464 - val_loss: 1.2671 - val_acc: 0.6372\nEpoch 131/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4331 - acc: 0.7491 - val_loss: 1.1674 - val_acc: 0.6407\nEpoch 132/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4329 - acc: 0.7477 - val_loss: 1.1537 - val_acc: 0.6462\nEpoch 133/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4305 - acc: 0.7502 - val_loss: 1.2038 - val_acc: 0.6432\nEpoch 134/200\n69420/69420 [==============================] - 13s 181us/step - loss: 0.4327 - acc: 0.7486 - val_loss: 1.1186 - val_acc: 0.6317\nEpoch 135/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4322 - acc: 0.7486 - val_loss: 1.1767 - val_acc: 0.6402\nEpoch 136/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4296 - acc: 0.7526 - val_loss: 1.0937 - val_acc: 0.6362\nEpoch 137/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4295 - acc: 0.7519 - val_loss: 1.2054 - val_acc: 0.6382\nEpoch 138/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4281 - acc: 0.7540 - val_loss: 1.2205 - val_acc: 0.6367\nEpoch 139/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4283 - acc: 0.7522 - val_loss: 1.1074 - val_acc: 0.6297\nEpoch 140/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4267 - acc: 0.7546 - val_loss: 1.1143 - val_acc: 0.6347\nEpoch 141/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4282 - acc: 0.7521 - val_loss: 1.2737 - val_acc: 0.6267\nEpoch 142/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4262 - acc: 0.7539 - val_loss: 1.2067 - val_acc: 0.6282\nEpoch 143/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4253 - acc: 0.7546 - val_loss: 1.2294 - val_acc: 0.6312\nEpoch 144/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4247 - acc: 0.7547 - val_loss: 1.1257 - val_acc: 0.6287\nEpoch 145/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4271 - acc: 0.7537 - val_loss: 1.2401 - val_acc: 0.6187\nEpoch 146/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4241 - acc: 0.7548 - val_loss: 1.2517 - val_acc: 0.6252\nEpoch 147/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4239 - acc: 0.7561 - val_loss: 1.2879 - val_acc: 0.6372\nEpoch 148/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4228 - acc: 0.7576 - val_loss: 1.3677 - val_acc: 0.6457\nEpoch 149/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4232 - acc: 0.7535 - val_loss: 1.2445 - val_acc: 0.6297\nEpoch 150/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4215 - acc: 0.7558 - val_loss: 1.1759 - val_acc: 0.6232\nEpoch 151/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4232 - acc: 0.7569 - val_loss: 1.2478 - val_acc: 0.6312\nEpoch 152/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4245 - acc: 0.7564 - val_loss: 1.2549 - val_acc: 0.6277\nEpoch 153/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4200 - acc: 0.7575 - val_loss: 1.2265 - val_acc: 0.6302\nEpoch 154/200\n69420/69420 [==============================] - 12s 178us/step - loss: 0.4183 - acc: 0.7583 - val_loss: 1.1629 - val_acc: 0.6217\nEpoch 155/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4208 - acc: 0.7575 - val_loss: 1.2909 - val_acc: 0.6352\nEpoch 156/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4184 - acc: 0.7582 - val_loss: 1.2823 - val_acc: 0.6332\nEpoch 157/200\n69420/69420 [==============================] - 13s 181us/step - loss: 0.4224 - acc: 0.7566 - val_loss: 1.2469 - val_acc: 0.6287\nEpoch 158/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4189 - acc: 0.7596 - val_loss: 1.2368 - val_acc: 0.6357\nEpoch 159/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4192 - acc: 0.7591 - val_loss: 1.2244 - val_acc: 0.6222\nEpoch 160/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4161 - acc: 0.7613 - val_loss: 1.2197 - val_acc: 0.6387\nEpoch 161/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4162 - acc: 0.7606 - val_loss: 1.2904 - val_acc: 0.6372\nEpoch 162/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4150 - acc: 0.7613 - val_loss: 1.1506 - val_acc: 0.6247\nEpoch 163/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4168 - acc: 0.7606 - val_loss: 1.3452 - val_acc: 0.6212\nEpoch 164/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4148 - acc: 0.7611 - val_loss: 1.2202 - val_acc: 0.6292\nEpoch 165/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4174 - acc: 0.7605 - val_loss: 1.4566 - val_acc: 0.6287\nEpoch 166/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4159 - acc: 0.7619 - val_loss: 1.3798 - val_acc: 0.6332\nEpoch 167/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4157 - acc: 0.7619 - val_loss: 1.3556 - val_acc: 0.6372\nEpoch 168/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4124 - acc: 0.7630 - val_loss: 1.2341 - val_acc: 0.6362\nEpoch 169/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4111 - acc: 0.7625 - val_loss: 1.2733 - val_acc: 0.6317\nEpoch 170/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4122 - acc: 0.7643 - val_loss: 1.3181 - val_acc: 0.6377\nEpoch 171/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4116 - acc: 0.7655 - val_loss: 1.3908 - val_acc: 0.6362\nEpoch 172/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4142 - acc: 0.7615 - val_loss: 1.2637 - val_acc: 0.6312\nEpoch 173/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4078 - acc: 0.7668 - val_loss: 1.2891 - val_acc: 0.6377\nEpoch 174/200\n69420/69420 [==============================] - 12s 178us/step - loss: 0.4118 - acc: 0.7658 - val_loss: 1.3604 - val_acc: 0.6242\nEpoch 175/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4089 - acc: 0.7664 - val_loss: 1.2488 - val_acc: 0.6407\nEpoch 176/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4093 - acc: 0.7655 - val_loss: 1.2561 - val_acc: 0.6317\nEpoch 177/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4095 - acc: 0.7659 - val_loss: 1.2376 - val_acc: 0.6222\nEpoch 178/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4097 - acc: 0.7654 - val_loss: 1.2352 - val_acc: 0.6157\nEpoch 179/200\n69420/69420 [==============================] - 12s 178us/step - loss: 0.4074 - acc: 0.7671 - val_loss: 1.3833 - val_acc: 0.6262\nEpoch 180/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4078 - acc: 0.7676 - val_loss: 1.2910 - val_acc: 0.6247\nEpoch 181/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4082 - acc: 0.7655 - val_loss: 1.4020 - val_acc: 0.6297\nEpoch 182/200\n69420/69420 [==============================] - 12s 178us/step - loss: 0.4078 - acc: 0.7677 - val_loss: 1.2579 - val_acc: 0.6437\nEpoch 183/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4034 - acc: 0.7699 - val_loss: 1.3964 - val_acc: 0.6362\nEpoch 184/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4046 - acc: 0.7699 - val_loss: 1.3412 - val_acc: 0.6307\nEpoch 185/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4046 - acc: 0.7689 - val_loss: 1.4976 - val_acc: 0.6387\nEpoch 186/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4047 - acc: 0.7681 - val_loss: 1.4790 - val_acc: 0.6362\nEpoch 187/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4058 - acc: 0.7684 - val_loss: 1.3005 - val_acc: 0.6302\nEpoch 188/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4064 - acc: 0.7694 - val_loss: 1.3672 - val_acc: 0.6337\nEpoch 189/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4049 - acc: 0.7685 - val_loss: 1.3175 - val_acc: 0.6367\nEpoch 190/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4031 - acc: 0.7714 - val_loss: 1.2893 - val_acc: 0.6232\nEpoch 191/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4032 - acc: 0.7687 - val_loss: 1.3258 - val_acc: 0.6247\nEpoch 192/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4014 - acc: 0.7719 - val_loss: 1.3031 - val_acc: 0.6307\nEpoch 193/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4015 - acc: 0.7709 - val_loss: 1.5133 - val_acc: 0.6287\nEpoch 194/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4023 - acc: 0.7709 - val_loss: 1.4070 - val_acc: 0.6242\nEpoch 195/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4020 - acc: 0.7718 - val_loss: 1.4798 - val_acc: 0.6342\nEpoch 196/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4022 - acc: 0.7705 - val_loss: 1.3685 - val_acc: 0.6312\nEpoch 197/200\n69420/69420 [==============================] - 13s 180us/step - loss: 0.4034 - acc: 0.7712 - val_loss: 1.2405 - val_acc: 0.6137\nEpoch 198/200\n69420/69420 [==============================] - 12s 179us/step - loss: 0.4000 - acc: 0.7733 - val_loss: 1.3355 - val_acc: 0.6262\nEpoch 199/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.4008 - acc: 0.7727 - val_loss: 1.2026 - val_acc: 0.6222\nEpoch 200/200\n69420/69420 [==============================] - 12s 180us/step - loss: 0.3975 - acc: 0.7762 - val_loss: 1.3712 - val_acc: 0.6217\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f6f6551b5f8>"},"metadata":{}}]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten,Dropout,Conv2D\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\n\nembedding_dim = 64\n\n# create the model\nmodel = Sequential()\nmodel.add(Embedding(len(tokenizer.word_index)+1, embedding_dim, input_length=max_length))\nmodel.add(Conv1D(filters=256, kernel_size=2, padding='same', activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv1D(filters=256, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=128, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=16, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\n\nmodel.add(Flatten())\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=200, batch_size=128)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T12:03:24.718576Z","iopub.execute_input":"2021-12-02T12:03:24.718900Z","iopub.status.idle":"2021-12-02T12:50:54.037114Z","shell.execute_reply.started":"2021-12-02T12:03:24.718847Z","shell.execute_reply":"2021-12-02T12:50:54.036495Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_4 (Embedding)      (None, 280, 64)           1344      \n_________________________________________________________________\nconv1d_15 (Conv1D)           (None, 280, 256)          33024     \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 280, 256)          0         \n_________________________________________________________________\nconv1d_16 (Conv1D)           (None, 280, 256)          131328    \n_________________________________________________________________\nmax_pooling1d_12 (MaxPooling (None, 280, 256)          0         \n_________________________________________________________________\nconv1d_17 (Conv1D)           (None, 280, 128)          65664     \n_________________________________________________________________\nmax_pooling1d_13 (MaxPooling (None, 280, 128)          0         \n_________________________________________________________________\nconv1d_18 (Conv1D)           (None, 280, 64)           16448     \n_________________________________________________________________\nmax_pooling1d_14 (MaxPooling (None, 280, 64)           0         \n_________________________________________________________________\nconv1d_19 (Conv1D)           (None, 280, 32)           4128      \n_________________________________________________________________\nmax_pooling1d_15 (MaxPooling (None, 280, 32)           0         \n_________________________________________________________________\nconv1d_20 (Conv1D)           (None, 280, 16)           1040      \n_________________________________________________________________\nmax_pooling1d_16 (MaxPooling (None, 280, 16)           0         \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 4480)              0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 8)                 35848     \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 8)                 0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 2)                 18        \n=================================================================\nTotal params: 288,842\nTrainable params: 288,842\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 69420 samples, validate on 2001 samples\nEpoch 1/200\n69420/69420 [==============================] - 15s 221us/step - loss: 0.6742 - acc: 0.5826 - val_loss: 0.6869 - val_acc: 0.5002\nEpoch 2/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.6585 - acc: 0.5871 - val_loss: 0.6560 - val_acc: 0.5542\nEpoch 3/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.6458 - acc: 0.5990 - val_loss: 0.6312 - val_acc: 0.6122\nEpoch 4/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.6342 - acc: 0.6181 - val_loss: 0.6301 - val_acc: 0.6007\nEpoch 5/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.6249 - acc: 0.6389 - val_loss: 0.6292 - val_acc: 0.6052\nEpoch 6/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.6174 - acc: 0.6503 - val_loss: 0.6378 - val_acc: 0.6097\nEpoch 7/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.6128 - acc: 0.6552 - val_loss: 0.6237 - val_acc: 0.6302\nEpoch 8/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.6087 - acc: 0.6605 - val_loss: 0.6392 - val_acc: 0.6122\nEpoch 9/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.6041 - acc: 0.6625 - val_loss: 0.6309 - val_acc: 0.6247\nEpoch 10/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.6015 - acc: 0.6644 - val_loss: 0.6247 - val_acc: 0.6222\nEpoch 11/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.5985 - acc: 0.6657 - val_loss: 0.6319 - val_acc: 0.6252\nEpoch 12/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.5931 - acc: 0.6697 - val_loss: 0.6218 - val_acc: 0.6297\nEpoch 13/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.5896 - acc: 0.6715 - val_loss: 0.6125 - val_acc: 0.6387\nEpoch 14/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.5873 - acc: 0.6733 - val_loss: 0.6273 - val_acc: 0.6302\nEpoch 15/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.5844 - acc: 0.6742 - val_loss: 0.6239 - val_acc: 0.6297\nEpoch 16/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.5817 - acc: 0.6751 - val_loss: 0.6258 - val_acc: 0.6262\nEpoch 17/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.5806 - acc: 0.6758 - val_loss: 0.6175 - val_acc: 0.6232\nEpoch 18/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.5762 - acc: 0.6795 - val_loss: 0.6244 - val_acc: 0.6347\nEpoch 19/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.5723 - acc: 0.6809 - val_loss: 0.6414 - val_acc: 0.6237\nEpoch 20/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.5721 - acc: 0.6799 - val_loss: 0.6518 - val_acc: 0.6097\nEpoch 21/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.5683 - acc: 0.6806 - val_loss: 0.6370 - val_acc: 0.6347\nEpoch 22/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.5671 - acc: 0.6822 - val_loss: 0.6502 - val_acc: 0.6142\nEpoch 23/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.5639 - acc: 0.6829 - val_loss: 0.6576 - val_acc: 0.6202\nEpoch 24/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.5632 - acc: 0.6835 - val_loss: 0.6502 - val_acc: 0.6357\nEpoch 25/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.5614 - acc: 0.6836 - val_loss: 0.6356 - val_acc: 0.6297\nEpoch 26/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.5585 - acc: 0.6847 - val_loss: 0.6661 - val_acc: 0.6142\nEpoch 27/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.5575 - acc: 0.6866 - val_loss: 0.6941 - val_acc: 0.6277\nEpoch 28/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.5526 - acc: 0.6873 - val_loss: 0.6757 - val_acc: 0.6162\nEpoch 29/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.5506 - acc: 0.6885 - val_loss: 0.6758 - val_acc: 0.6147\nEpoch 30/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.5482 - acc: 0.6877 - val_loss: 0.6602 - val_acc: 0.6127\nEpoch 31/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.5449 - acc: 0.6894 - val_loss: 0.7051 - val_acc: 0.6097\nEpoch 32/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.5443 - acc: 0.6903 - val_loss: 0.6702 - val_acc: 0.6137\nEpoch 33/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.5418 - acc: 0.6905 - val_loss: 0.6849 - val_acc: 0.6137\nEpoch 34/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.5384 - acc: 0.6913 - val_loss: 0.6839 - val_acc: 0.6187\nEpoch 35/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.5369 - acc: 0.6913 - val_loss: 0.6898 - val_acc: 0.6177\nEpoch 36/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.5332 - acc: 0.6932 - val_loss: 0.6519 - val_acc: 0.6277\nEpoch 37/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.5327 - acc: 0.6937 - val_loss: 0.7087 - val_acc: 0.6102\nEpoch 38/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.5286 - acc: 0.6947 - val_loss: 0.7281 - val_acc: 0.6327\nEpoch 39/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.5281 - acc: 0.6938 - val_loss: 0.7374 - val_acc: 0.6162\nEpoch 40/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.5269 - acc: 0.6971 - val_loss: 0.6835 - val_acc: 0.6362\nEpoch 41/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.5246 - acc: 0.6938 - val_loss: 0.7257 - val_acc: 0.6077\nEpoch 42/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.5230 - acc: 0.6965 - val_loss: 0.7073 - val_acc: 0.6232\nEpoch 43/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.5203 - acc: 0.6963 - val_loss: 0.7459 - val_acc: 0.6092\nEpoch 44/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.5199 - acc: 0.6969 - val_loss: 0.7466 - val_acc: 0.6137\nEpoch 45/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.5158 - acc: 0.7011 - val_loss: 0.7446 - val_acc: 0.6212\nEpoch 46/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.5171 - acc: 0.6978 - val_loss: 0.7297 - val_acc: 0.6147\nEpoch 47/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.5136 - acc: 0.6997 - val_loss: 0.7330 - val_acc: 0.6187\nEpoch 48/200\n69420/69420 [==============================] - 14s 208us/step - loss: 0.5118 - acc: 0.6999 - val_loss: 0.7540 - val_acc: 0.6147\nEpoch 49/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.5100 - acc: 0.6989 - val_loss: 0.7404 - val_acc: 0.6122\nEpoch 50/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.5090 - acc: 0.7010 - val_loss: 0.7618 - val_acc: 0.6152\nEpoch 51/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.5081 - acc: 0.7019 - val_loss: 0.7152 - val_acc: 0.6152\nEpoch 52/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.5057 - acc: 0.7016 - val_loss: 0.7529 - val_acc: 0.6187\nEpoch 53/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.5037 - acc: 0.7020 - val_loss: 0.7615 - val_acc: 0.6077\nEpoch 54/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.5030 - acc: 0.7031 - val_loss: 0.7485 - val_acc: 0.6092\nEpoch 55/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.5007 - acc: 0.7023 - val_loss: 0.7786 - val_acc: 0.6122\nEpoch 56/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4983 - acc: 0.7036 - val_loss: 0.7860 - val_acc: 0.6182\nEpoch 57/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4991 - acc: 0.7045 - val_loss: 0.7705 - val_acc: 0.6082\nEpoch 58/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4972 - acc: 0.7056 - val_loss: 0.7867 - val_acc: 0.6257\nEpoch 59/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4956 - acc: 0.7068 - val_loss: 0.8172 - val_acc: 0.6027\nEpoch 60/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4939 - acc: 0.7062 - val_loss: 0.7681 - val_acc: 0.6152\nEpoch 61/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4924 - acc: 0.7064 - val_loss: 0.7681 - val_acc: 0.6057\nEpoch 62/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.4913 - acc: 0.7078 - val_loss: 0.8116 - val_acc: 0.6082\nEpoch 63/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4880 - acc: 0.7090 - val_loss: 0.7601 - val_acc: 0.6162\nEpoch 64/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4877 - acc: 0.7092 - val_loss: 0.7798 - val_acc: 0.6227\nEpoch 65/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4889 - acc: 0.7066 - val_loss: 0.8255 - val_acc: 0.6142\nEpoch 66/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4856 - acc: 0.7091 - val_loss: 0.8214 - val_acc: 0.6157\nEpoch 67/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4838 - acc: 0.7108 - val_loss: 0.8133 - val_acc: 0.6047\nEpoch 68/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4832 - acc: 0.7100 - val_loss: 0.8150 - val_acc: 0.6117\nEpoch 69/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4806 - acc: 0.7102 - val_loss: 0.8181 - val_acc: 0.6092\nEpoch 70/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4820 - acc: 0.7114 - val_loss: 0.7967 - val_acc: 0.6142\nEpoch 71/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4785 - acc: 0.7124 - val_loss: 0.8228 - val_acc: 0.6127\nEpoch 72/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4809 - acc: 0.7100 - val_loss: 0.8123 - val_acc: 0.6157\nEpoch 73/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4783 - acc: 0.7116 - val_loss: 0.8435 - val_acc: 0.6107\nEpoch 74/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4760 - acc: 0.7127 - val_loss: 0.8430 - val_acc: 0.6242\nEpoch 75/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4744 - acc: 0.7128 - val_loss: 0.8432 - val_acc: 0.6177\nEpoch 76/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4773 - acc: 0.7116 - val_loss: 0.8444 - val_acc: 0.6172\nEpoch 77/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4754 - acc: 0.7125 - val_loss: 0.7975 - val_acc: 0.6192\nEpoch 78/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.4738 - acc: 0.7131 - val_loss: 0.8333 - val_acc: 0.6112\nEpoch 79/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4720 - acc: 0.7134 - val_loss: 0.8476 - val_acc: 0.6142\nEpoch 80/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4706 - acc: 0.7146 - val_loss: 0.8592 - val_acc: 0.6162\nEpoch 81/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4695 - acc: 0.7132 - val_loss: 0.8364 - val_acc: 0.6122\nEpoch 82/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4689 - acc: 0.7140 - val_loss: 0.8972 - val_acc: 0.6222\nEpoch 83/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4703 - acc: 0.7151 - val_loss: 0.9068 - val_acc: 0.6082\nEpoch 84/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4661 - acc: 0.7139 - val_loss: 0.8793 - val_acc: 0.6042\nEpoch 85/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4669 - acc: 0.7158 - val_loss: 0.8865 - val_acc: 0.6117\nEpoch 86/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4672 - acc: 0.7156 - val_loss: 0.8589 - val_acc: 0.6222\nEpoch 87/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4654 - acc: 0.7169 - val_loss: 0.9275 - val_acc: 0.6147\nEpoch 88/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4650 - acc: 0.7161 - val_loss: 0.9864 - val_acc: 0.6147\nEpoch 89/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4648 - acc: 0.7155 - val_loss: 0.9392 - val_acc: 0.6087\nEpoch 90/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4628 - acc: 0.7173 - val_loss: 0.9817 - val_acc: 0.6197\nEpoch 91/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4607 - acc: 0.7169 - val_loss: 0.8920 - val_acc: 0.6017\nEpoch 92/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4628 - acc: 0.7169 - val_loss: 0.9271 - val_acc: 0.6172\nEpoch 93/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4619 - acc: 0.7182 - val_loss: 0.9425 - val_acc: 0.6082\nEpoch 94/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4621 - acc: 0.7159 - val_loss: 0.9135 - val_acc: 0.6082\nEpoch 95/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4596 - acc: 0.7182 - val_loss: 0.8472 - val_acc: 0.6062\nEpoch 96/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4577 - acc: 0.7179 - val_loss: 0.9456 - val_acc: 0.6257\nEpoch 97/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4589 - acc: 0.7179 - val_loss: 0.9059 - val_acc: 0.6067\nEpoch 98/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4610 - acc: 0.7174 - val_loss: 0.9708 - val_acc: 0.6077\nEpoch 99/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.4564 - acc: 0.7202 - val_loss: 0.9418 - val_acc: 0.6097\nEpoch 100/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4581 - acc: 0.7178 - val_loss: 0.9565 - val_acc: 0.6192\nEpoch 101/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4573 - acc: 0.7193 - val_loss: 0.9670 - val_acc: 0.6077\nEpoch 102/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4525 - acc: 0.7205 - val_loss: 0.9136 - val_acc: 0.6107\nEpoch 103/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4538 - acc: 0.7209 - val_loss: 0.9836 - val_acc: 0.6127\nEpoch 104/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4538 - acc: 0.7193 - val_loss: 1.0003 - val_acc: 0.6107\nEpoch 105/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4552 - acc: 0.7203 - val_loss: 0.9487 - val_acc: 0.6097\nEpoch 106/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.4540 - acc: 0.7201 - val_loss: 0.9598 - val_acc: 0.6197\nEpoch 107/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4531 - acc: 0.7205 - val_loss: 0.9742 - val_acc: 0.6142\nEpoch 108/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4523 - acc: 0.7210 - val_loss: 0.9903 - val_acc: 0.6072\nEpoch 109/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4489 - acc: 0.7220 - val_loss: 0.9098 - val_acc: 0.6157\nEpoch 110/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4516 - acc: 0.7213 - val_loss: 1.0569 - val_acc: 0.6032\nEpoch 111/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4506 - acc: 0.7217 - val_loss: 0.9886 - val_acc: 0.6192\nEpoch 112/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4521 - acc: 0.7205 - val_loss: 1.0527 - val_acc: 0.6142\nEpoch 113/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.4486 - acc: 0.7220 - val_loss: 1.0639 - val_acc: 0.5997\nEpoch 114/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4471 - acc: 0.7226 - val_loss: 1.0405 - val_acc: 0.6062\nEpoch 115/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4460 - acc: 0.7220 - val_loss: 1.0067 - val_acc: 0.6117\nEpoch 116/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4471 - acc: 0.7217 - val_loss: 1.0063 - val_acc: 0.6112\nEpoch 117/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4490 - acc: 0.7208 - val_loss: 0.9983 - val_acc: 0.6092\nEpoch 118/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4461 - acc: 0.7252 - val_loss: 1.0880 - val_acc: 0.6092\nEpoch 119/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4460 - acc: 0.7222 - val_loss: 1.0724 - val_acc: 0.6107\nEpoch 120/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.4436 - acc: 0.7221 - val_loss: 1.0970 - val_acc: 0.6147\nEpoch 121/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4454 - acc: 0.7219 - val_loss: 1.0617 - val_acc: 0.6092\nEpoch 122/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4471 - acc: 0.7223 - val_loss: 1.0200 - val_acc: 0.6122\nEpoch 123/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4440 - acc: 0.7236 - val_loss: 1.0243 - val_acc: 0.6177\nEpoch 124/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4425 - acc: 0.7251 - val_loss: 1.0954 - val_acc: 0.6192\nEpoch 125/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4447 - acc: 0.7239 - val_loss: 1.0051 - val_acc: 0.6137\nEpoch 126/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4421 - acc: 0.7259 - val_loss: 1.0564 - val_acc: 0.6027\nEpoch 127/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4406 - acc: 0.7230 - val_loss: 1.2080 - val_acc: 0.6027\nEpoch 128/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4439 - acc: 0.7242 - val_loss: 1.0154 - val_acc: 0.6082\nEpoch 129/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4419 - acc: 0.7246 - val_loss: 1.1054 - val_acc: 0.6137\nEpoch 130/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4418 - acc: 0.7259 - val_loss: 1.0285 - val_acc: 0.6037\nEpoch 131/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4414 - acc: 0.7252 - val_loss: 1.0996 - val_acc: 0.6007\nEpoch 132/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4375 - acc: 0.7259 - val_loss: 1.1862 - val_acc: 0.6017\nEpoch 133/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4394 - acc: 0.7277 - val_loss: 1.0014 - val_acc: 0.6072\nEpoch 134/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4388 - acc: 0.7269 - val_loss: 1.0469 - val_acc: 0.6162\nEpoch 135/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4384 - acc: 0.7245 - val_loss: 1.1276 - val_acc: 0.5957\nEpoch 136/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4389 - acc: 0.7268 - val_loss: 1.1263 - val_acc: 0.6087\nEpoch 137/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4382 - acc: 0.7258 - val_loss: 1.1363 - val_acc: 0.6067\nEpoch 138/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4392 - acc: 0.7249 - val_loss: 1.0866 - val_acc: 0.6067\nEpoch 139/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4374 - acc: 0.7258 - val_loss: 1.0372 - val_acc: 0.6052\nEpoch 140/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4384 - acc: 0.7244 - val_loss: 1.0522 - val_acc: 0.6047\nEpoch 141/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4364 - acc: 0.7251 - val_loss: 1.1526 - val_acc: 0.6122\nEpoch 142/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4374 - acc: 0.7267 - val_loss: 1.0440 - val_acc: 0.6067\nEpoch 143/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.4364 - acc: 0.7285 - val_loss: 1.0980 - val_acc: 0.6127\nEpoch 144/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4314 - acc: 0.7287 - val_loss: 1.1200 - val_acc: 0.6132\nEpoch 145/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4350 - acc: 0.7278 - val_loss: 1.0971 - val_acc: 0.6152\nEpoch 146/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4357 - acc: 0.7289 - val_loss: 1.0825 - val_acc: 0.6122\nEpoch 147/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4337 - acc: 0.7293 - val_loss: 1.0270 - val_acc: 0.6162\nEpoch 148/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4349 - acc: 0.7275 - val_loss: 1.0753 - val_acc: 0.6247\nEpoch 149/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4336 - acc: 0.7294 - val_loss: 1.1324 - val_acc: 0.6162\nEpoch 150/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.4342 - acc: 0.7281 - val_loss: 1.0268 - val_acc: 0.6152\nEpoch 151/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4341 - acc: 0.7268 - val_loss: 1.0988 - val_acc: 0.6237\nEpoch 152/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4329 - acc: 0.7276 - val_loss: 1.1364 - val_acc: 0.6092\nEpoch 153/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4310 - acc: 0.7288 - val_loss: 1.0973 - val_acc: 0.6157\nEpoch 154/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.4308 - acc: 0.7286 - val_loss: 1.1498 - val_acc: 0.6127\nEpoch 155/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4317 - acc: 0.7281 - val_loss: 1.0874 - val_acc: 0.6147\nEpoch 156/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4314 - acc: 0.7300 - val_loss: 1.1508 - val_acc: 0.6142\nEpoch 157/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.4326 - acc: 0.7304 - val_loss: 1.0394 - val_acc: 0.6107\nEpoch 158/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4311 - acc: 0.7283 - val_loss: 0.9814 - val_acc: 0.6102\nEpoch 159/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.4289 - acc: 0.7295 - val_loss: 1.0468 - val_acc: 0.6127\nEpoch 160/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4294 - acc: 0.7315 - val_loss: 1.1887 - val_acc: 0.6062\nEpoch 161/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.4296 - acc: 0.7305 - val_loss: 1.1344 - val_acc: 0.6122\nEpoch 162/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4294 - acc: 0.7289 - val_loss: 1.1484 - val_acc: 0.6052\nEpoch 163/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4304 - acc: 0.7303 - val_loss: 1.2248 - val_acc: 0.6077\nEpoch 164/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4304 - acc: 0.7286 - val_loss: 0.9384 - val_acc: 0.6082\nEpoch 165/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4293 - acc: 0.7301 - val_loss: 1.1678 - val_acc: 0.6077\nEpoch 166/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4283 - acc: 0.7313 - val_loss: 1.2438 - val_acc: 0.6187\nEpoch 167/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4277 - acc: 0.7311 - val_loss: 1.1056 - val_acc: 0.6157\nEpoch 168/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4289 - acc: 0.7307 - val_loss: 1.1004 - val_acc: 0.6197\nEpoch 169/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4293 - acc: 0.7308 - val_loss: 1.2915 - val_acc: 0.6047\nEpoch 170/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4247 - acc: 0.7309 - val_loss: 1.1627 - val_acc: 0.6127\nEpoch 171/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4285 - acc: 0.7309 - val_loss: 1.3281 - val_acc: 0.5987\nEpoch 172/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4284 - acc: 0.7303 - val_loss: 1.4343 - val_acc: 0.5997\nEpoch 173/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4277 - acc: 0.7298 - val_loss: 1.1143 - val_acc: 0.6127\nEpoch 174/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4268 - acc: 0.7311 - val_loss: 1.1830 - val_acc: 0.6197\nEpoch 175/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4258 - acc: 0.7332 - val_loss: 1.1961 - val_acc: 0.6067\nEpoch 176/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4237 - acc: 0.7316 - val_loss: 1.2898 - val_acc: 0.6062\nEpoch 177/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.4255 - acc: 0.7321 - val_loss: 1.1484 - val_acc: 0.6092\nEpoch 178/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4240 - acc: 0.7324 - val_loss: 1.0946 - val_acc: 0.6202\nEpoch 179/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4252 - acc: 0.7324 - val_loss: 1.1225 - val_acc: 0.6147\nEpoch 180/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4250 - acc: 0.7312 - val_loss: 1.3074 - val_acc: 0.6212\nEpoch 181/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4239 - acc: 0.7309 - val_loss: 1.1610 - val_acc: 0.6067\nEpoch 182/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4238 - acc: 0.7322 - val_loss: 1.3619 - val_acc: 0.6222\nEpoch 183/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4244 - acc: 0.7326 - val_loss: 1.1663 - val_acc: 0.6067\nEpoch 184/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.4233 - acc: 0.7310 - val_loss: 1.0102 - val_acc: 0.6142\nEpoch 185/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4254 - acc: 0.7314 - val_loss: 1.1333 - val_acc: 0.6112\nEpoch 186/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4201 - acc: 0.7351 - val_loss: 1.1272 - val_acc: 0.6207\nEpoch 187/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4256 - acc: 0.7311 - val_loss: 1.1481 - val_acc: 0.6202\nEpoch 188/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4251 - acc: 0.7310 - val_loss: 1.2021 - val_acc: 0.6142\nEpoch 189/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.4199 - acc: 0.7351 - val_loss: 1.2878 - val_acc: 0.6097\nEpoch 190/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4247 - acc: 0.7330 - val_loss: 1.1516 - val_acc: 0.6097\nEpoch 191/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.4217 - acc: 0.7338 - val_loss: 1.1354 - val_acc: 0.6112\nEpoch 192/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4227 - acc: 0.7344 - val_loss: 1.1534 - val_acc: 0.6137\nEpoch 193/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4223 - acc: 0.7332 - val_loss: 1.2401 - val_acc: 0.6107\nEpoch 194/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4229 - acc: 0.7343 - val_loss: 1.0002 - val_acc: 0.6097\nEpoch 195/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4230 - acc: 0.7334 - val_loss: 1.1683 - val_acc: 0.6202\nEpoch 196/200\n69420/69420 [==============================] - 14s 206us/step - loss: 0.4204 - acc: 0.7338 - val_loss: 1.2063 - val_acc: 0.6082\nEpoch 197/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4190 - acc: 0.7349 - val_loss: 1.1511 - val_acc: 0.6057\nEpoch 198/200\n69420/69420 [==============================] - 14s 207us/step - loss: 0.4213 - acc: 0.7342 - val_loss: 1.1910 - val_acc: 0.6097\nEpoch 199/200\n69420/69420 [==============================] - 14s 204us/step - loss: 0.4199 - acc: 0.7337 - val_loss: 1.2517 - val_acc: 0.6007\nEpoch 200/200\n69420/69420 [==============================] - 14s 205us/step - loss: 0.4204 - acc: 0.7332 - val_loss: 1.0519 - val_acc: 0.6177\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f6f63e15390>"},"metadata":{}}]},{"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(model, to_file='model.png')","metadata":{"execution":{"iopub.status.busy":"2021-12-02T12:54:21.836601Z","iopub.execute_input":"2021-12-02T12:54:21.836884Z","iopub.status.idle":"2021-12-02T12:54:22.877489Z","shell.execute_reply.started":"2021-12-02T12:54:21.836834Z","shell.execute_reply":"2021-12-02T12:54:22.876511Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten,Dropout,Conv2D\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\n\nembedding_dim = 64\n\n# create the model\nmodel = Sequential()\nmodel.add(Embedding(len(tokenizer.word_index)+1, embedding_dim, input_length=max_length))\nmodel.add(Conv1D(filters=512, kernel_size=2, padding='same', activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv1D(filters=512, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=256, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=128, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=16, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\n\nmodel.add(Flatten())\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=200, batch_size=512)\n#embedding_dim = 64ï¼Œepochs=200, batch_size=512","metadata":{"execution":{"iopub.status.busy":"2021-12-02T12:54:56.789507Z","iopub.execute_input":"2021-12-02T12:54:56.789805Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_6 (Embedding)      (None, 280, 64)           1344      \n_________________________________________________________________\nconv1d_28 (Conv1D)           (None, 280, 512)          66048     \n_________________________________________________________________\ndropout_11 (Dropout)         (None, 280, 512)          0         \n_________________________________________________________________\nconv1d_29 (Conv1D)           (None, 280, 512)          524800    \n_________________________________________________________________\nmax_pooling1d_23 (MaxPooling (None, 280, 512)          0         \n_________________________________________________________________\nconv1d_30 (Conv1D)           (None, 280, 256)          262400    \n_________________________________________________________________\nmax_pooling1d_24 (MaxPooling (None, 280, 256)          0         \n_________________________________________________________________\nconv1d_31 (Conv1D)           (None, 280, 128)          65664     \n_________________________________________________________________\nmax_pooling1d_25 (MaxPooling (None, 280, 128)          0         \n_________________________________________________________________\nconv1d_32 (Conv1D)           (None, 280, 64)           16448     \n_________________________________________________________________\nmax_pooling1d_26 (MaxPooling (None, 280, 64)           0         \n_________________________________________________________________\nconv1d_33 (Conv1D)           (None, 280, 32)           4128      \n_________________________________________________________________\nmax_pooling1d_27 (MaxPooling (None, 280, 32)           0         \n_________________________________________________________________\nconv1d_34 (Conv1D)           (None, 280, 16)           1040      \n_________________________________________________________________\nmax_pooling1d_28 (MaxPooling (None, 280, 16)           0         \n_________________________________________________________________\nflatten_6 (Flatten)          (None, 4480)              0         \n_________________________________________________________________\ndense_11 (Dense)             (None, 8)                 35848     \n_________________________________________________________________\ndropout_12 (Dropout)         (None, 8)                 0         \n_________________________________________________________________\ndense_12 (Dense)             (None, 2)                 18        \n=================================================================\nTotal params: 977,738\nTrainable params: 977,738\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 69420 samples, validate on 2001 samples\nEpoch 1/200\n69420/69420 [==============================] - 31s 446us/step - loss: 0.6750 - acc: 0.5822 - val_loss: 0.6839 - val_acc: 0.5002\nEpoch 2/200\n69420/69420 [==============================] - 29s 422us/step - loss: 0.6617 - acc: 0.5826 - val_loss: 0.6770 - val_acc: 0.5002\nEpoch 3/200\n69420/69420 [==============================] - 29s 422us/step - loss: 0.6479 - acc: 0.6092 - val_loss: 0.6565 - val_acc: 0.6297\nEpoch 4/200\n69420/69420 [==============================] - 29s 422us/step - loss: 0.6379 - acc: 0.6312 - val_loss: 0.6507 - val_acc: 0.6222\nEpoch 5/200\n69420/69420 [==============================] - 29s 422us/step - loss: 0.6326 - acc: 0.6362 - val_loss: 0.6417 - val_acc: 0.6397\nEpoch 6/200\n69420/69420 [==============================] - 29s 423us/step - loss: 0.6323 - acc: 0.6397 - val_loss: 0.6344 - val_acc: 0.6507\nEpoch 7/200\n69420/69420 [==============================] - 29s 422us/step - loss: 0.6268 - acc: 0.6475 - val_loss: 0.6464 - val_acc: 0.6502\nEpoch 8/200\n69420/69420 [==============================] - 29s 423us/step - loss: 0.6257 - acc: 0.6508 - val_loss: 0.6426 - val_acc: 0.6562\nEpoch 9/200\n69420/69420 [==============================] - 29s 422us/step - loss: 0.6273 - acc: 0.6510 - val_loss: 0.6336 - val_acc: 0.6562\nEpoch 10/200\n69420/69420 [==============================] - 29s 422us/step - loss: 0.6237 - acc: 0.6540 - val_loss: 0.6363 - val_acc: 0.6677\nEpoch 11/200\n69420/69420 [==============================] - 29s 423us/step - loss: 0.6189 - acc: 0.6617 - val_loss: 0.6297 - val_acc: 0.6802\nEpoch 12/200\n69420/69420 [==============================] - 29s 422us/step - loss: 0.6129 - acc: 0.6684 - val_loss: 0.6270 - val_acc: 0.6817\nEpoch 13/200\n69420/69420 [==============================] - 29s 423us/step - loss: 0.6063 - acc: 0.6710 - val_loss: 0.6324 - val_acc: 0.6822\nEpoch 14/200\n69420/69420 [==============================] - 29s 423us/step - loss: 0.6039 - acc: 0.6741 - val_loss: 0.6122 - val_acc: 0.6717\nEpoch 15/200\n69420/69420 [==============================] - 29s 422us/step - loss: 0.5985 - acc: 0.6788 - val_loss: 0.6245 - val_acc: 0.6747\nEpoch 16/200\n69420/69420 [==============================] - 29s 422us/step - loss: 0.5938 - acc: 0.6800 - val_loss: 0.6053 - val_acc: 0.6742\nEpoch 17/200\n69420/69420 [==============================] - 29s 422us/step - loss: 0.5893 - acc: 0.6842 - val_loss: 0.6154 - val_acc: 0.6707\nEpoch 18/200\n69420/69420 [==============================] - 29s 423us/step - loss: 0.5856 - acc: 0.6872 - val_loss: 0.6061 - val_acc: 0.6772\nEpoch 19/200\n69420/69420 [==============================] - 29s 423us/step - loss: 0.5814 - acc: 0.6880 - val_loss: 0.6036 - val_acc: 0.6652\nEpoch 20/200\n69420/69420 [==============================] - 29s 422us/step - loss: 0.5777 - acc: 0.6895 - val_loss: 0.6058 - val_acc: 0.6702\nEpoch 21/200\n69420/69420 [==============================] - 29s 423us/step - loss: 0.5765 - acc: 0.6907 - val_loss: 0.6264 - val_acc: 0.6597\nEpoch 22/200\n69420/69420 [==============================] - 29s 422us/step - loss: 0.5737 - acc: 0.6914 - val_loss: 0.6242 - val_acc: 0.6657\nEpoch 23/200\n69420/69420 [==============================] - 29s 423us/step - loss: 0.5730 - acc: 0.6942 - val_loss: 0.6190 - val_acc: 0.6747\nEpoch 24/200\n69420/69420 [==============================] - 29s 423us/step - loss: 0.5647 - acc: 0.6980 - val_loss: 0.6073 - val_acc: 0.6702\nEpoch 25/200\n69420/69420 [==============================] - 29s 422us/step - loss: 0.5640 - acc: 0.6986 - val_loss: 0.6315 - val_acc: 0.6677\nEpoch 26/200\n20480/69420 [=======>......................] - ETA: 20s - loss: 0.5537 - acc: 0.7062","output_type":"stream"}]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten,Dropout,Conv2D\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\n\nembedding_dim = 64\n\n# create the model\nmodel = Sequential()\nmodel.add(Embedding(len(tokenizer.word_index)+1, embedding_dim, input_length=max_length))\nmodel.add(Conv1D(filters=512, kernel_size=2, padding='same', activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv1D(filters=512, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=256, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=128, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=16, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\n\nmodel.add(Flatten())\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=200, batch_size=256)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T10:24:40.347167Z","iopub.status.idle":"2021-12-02T10:24:40.347858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten,Dropout,Conv2D\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\n\nembedding_dim = 64\n\n# create the model\nmodel = Sequential()\nmodel.add(Embedding(len(tokenizer.word_index)+1, embedding_dim, input_length=max_length))\nmodel.add(Conv1D(filters=512, kernel_size=2, padding='same', activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv1D(filters=512, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=256, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=128, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\nmodel.add(Conv1D(filters=16, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=1))\n\nmodel.add(Flatten())\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=200, batch_size=128)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T10:24:40.347167Z","iopub.status.idle":"2021-12-02T10:24:40.347858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(model, to_file='model.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport numpy as np\n\n\nprint(classification_report(Y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.callbacks import LambdaCallback\nfrom keras.layers import Conv1D, Flatten\nfrom keras.layers import Dense ,Dropout,BatchNormalization\nfrom keras.models import Sequential \nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical \nfrom keras import regularizers\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import  VotingClassifier\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn import metrics\nfrom sklearn import ensemble\nfrom sklearn import gaussian_process\nfrom sklearn import linear_model\nfrom sklearn import naive_bayes\nfrom sklearn import neighbors\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn import discriminant_analysis\nfrom sklearn import model_selection\nfrom xgboost.sklearn import XGBClassifier \n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-02T08:21:39.265109Z","iopub.execute_input":"2021-12-02T08:21:39.265494Z","iopub.status.idle":"2021-12-02T08:21:39.496369Z","shell.execute_reply.started":"2021-12-02T08:21:39.265427Z","shell.execute_reply":"2021-12-02T08:21:39.495404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trying different Classifiers","metadata":{}},{"cell_type":"code","source":"#Machine Learning Algorithm (MLA) Selection and Initialization\n\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n    XGBClassifier()    \n    ]\n\n\n\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy' ]\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n   # cv_results = model_selection.cross_validate(alg, X_train, y_train)\n    alg.fit(x_train, y_train)\n    y_pred=alg.predict(x_test)\n    score=metrics.accuracy_score(y_test, y_pred)\n    \n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] =score\n\n    \n    \n    row_index+=1\n\n    \n\nMLA_compare\n\nMLA_compare.to_csv(\"classifier.csv\")\n#MLA_predict","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:21:26.264794Z","iopub.execute_input":"2021-12-01T08:21:26.265083Z","iopub.status.idle":"2021-12-01T08:21:26.388672Z","shell.execute_reply.started":"2021-12-01T08:21:26.265035Z","shell.execute_reply":"2021-12-01T08:21:26.387376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# One Hot Encoder","metadata":{}},{"cell_type":"code","source":"X = train_data.Sequence.astype(str).str[0:240]\nX = X.values\nprint(len(X[1]))\nprint(\"Every String has length equal to =\",len(X[1]))\n# Every Sequence Length is now 280","metadata":{"execution":{"iopub.status.busy":"2021-12-02T08:23:36.087975Z","iopub.execute_input":"2021-12-02T08:23:36.088743Z","iopub.status.idle":"2021-12-02T08:23:36.189886Z","shell.execute_reply.started":"2021-12-02T08:23:36.088669Z","shell.execute_reply":"2021-12-02T08:23:36.187199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def onehot(ltr):\n     return [1 if i==ord(ltr) else 0]\n\ndef onehotvec(s):\n     return [onehot(c) for c in list(s.lower())]\n\nsequence_encode=[]\n\nfor i in range(0,len(X)):\n    \n    X[i]=X[i].lower()\n    a=onehotvec(X[i])\n    a=np.asarray(a)\n    sequence_encode.append(a)\n    \nsequence_encode=np.asarray(sequence_encode)  \nprint(\"Shape of One Hot Encoded Sequence\",sequence_encode.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T08:24:01.70238Z","iopub.execute_input":"2021-12-02T08:24:01.702858Z","iopub.status.idle":"2021-12-02T08:24:16.713162Z","shell.execute_reply.started":"2021-12-02T08:24:01.702771Z","shell.execute_reply":"2021-12-02T08:24:16.711972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=sequence_encode.reshape(69420,240,24469,1)\nX.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-02T08:28:15.346315Z","iopub.execute_input":"2021-12-02T08:28:15.346739Z","iopub.status.idle":"2021-12-02T08:28:15.371516Z","shell.execute_reply.started":"2021-12-02T08:28:15.346669Z","shell.execute_reply":"2021-12-02T08:28:15.369436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conv 2D Training","metadata":{}},{"cell_type":"code","source":"from keras.layers import Conv2D,LeakyReLU,MaxPooling2D\nfrom keras.layers.core import Activation\n\n# create the model\nmodel = Sequential()\nmodel.add(Conv2D(16,kernel_size = (2,2),input_shape=(96,26,1)))\nmodel.add(Activation(\"relu\"))\nmodel.add(Conv2D(32,kernel_size = (2,2),input_shape=(96,26,1)))\nmodel.add(Activation(\"relu\"))\n\nmodel.add(MaxPooling2D(pool_size=(2, 2),strides=2, padding='same', data_format=None))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(64,kernel_size = (2,2),input_shape=(96,26,1)))\nmodel.add(Activation(\"relu\"))\nmodel.add(Conv2D(82,kernel_size = (2,2),input_shape=(96,26,1)))\nmodel.add(Activation(\"relu\"))\n\n\n\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\n\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=30, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T08:22:39.823774Z","iopub.execute_input":"2021-12-02T08:22:39.824185Z","iopub.status.idle":"2021-12-02T08:22:40.103239Z","shell.execute_reply.started":"2021-12-02T08:22:39.824118Z","shell.execute_reply":"2021-12-02T08:22:40.101543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Siamese Neural Network (Clustring ALgorithm)","metadata":{}},{"cell_type":"code","source":"# Import Keras and other Deep Learning dependencies\nfrom keras.models import Sequential\nimport time\nfrom keras.optimizers import Adam\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\nfrom keras.models import Model\nimport seaborn as sns\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D, AveragePooling2D\nfrom keras.layers.merge import Concatenate\nfrom keras.layers.core import Lambda, Flatten, Dense\nfrom keras.initializers import glorot_uniform\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.optimizers import *\nfrom keras.engine.topology import Layer\nfrom keras import backend as K\nfrom keras.regularizers import l2\nK.set_image_data_format('channels_last')\nimport cv2\nimport os\nfrom skimage import io\nimport numpy as np\nfrom numpy import genfromtxt\nimport pandas as pd\nimport tensorflow as tf\n\nimport numpy.random as rng\nfrom sklearn.utils import shuffle\n\n%matplotlib inline\n%load_ext autoreload\n%reload_ext autoreload\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=X_train.reshape(X_train.shape[0],96,26,1)\nX_test=X_test.reshape(X_test.shape[0],96,26,1)\ntrain_groups = [X_train[np.where(y_train==i)[0]] for i in np.unique(y_train)]\ntest_groups = [X_test[np.where(y_test==i)[0]] for i in np.unique(y_test)]\nprint('train groups:', [X.shape[0] for X in train_groups])\nprint('test groups:', [X.shape[0] for X in test_groups])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gen_random_batch(in_groups, batch_halfsize = 8):\n    out_img_a, out_img_b, out_score = [], [], []\n    all_groups = list(range(len(in_groups)))\n    for match_group in [True, False]:\n        group_idx = np.random.choice(all_groups, size = batch_halfsize)\n        out_img_a += [in_groups[c_idx][np.random.choice(range(in_groups[c_idx].shape[0]))] for c_idx in group_idx]\n        if match_group:\n            b_group_idx = group_idx\n            out_score += [1]*batch_halfsize\n        else:\n            # anything but the same group\n            non_group_idx = [np.random.choice([i for i in all_groups if i!=c_idx]) for c_idx in group_idx] \n            b_group_idx = non_group_idx\n            out_score += [0]*batch_halfsize\n            \n        out_img_b += [in_groups[c_idx][np.random.choice(range(in_groups[c_idx].shape[0]))] for c_idx in b_group_idx]\n            \n    return np.stack(out_img_a,0), np.stack(out_img_b,0), np.stack(out_score,0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Conv2D, BatchNormalization, MaxPool2D, Activation, Flatten, Dense, Dropout\nimg_in = Input(shape = X_train.shape[1:], name = 'FeatureNet_ImageInput')\nn_layer = img_in\nfor i in range(2):\n    n_layer = Conv2D(8*2**i, kernel_size = (3,3), activation = 'linear')(n_layer)\n    n_layer = BatchNormalization()(n_layer)\n    n_layer = Activation('relu')(n_layer)\n    n_layer = Conv2D(16*2**i, kernel_size = (3,3), activation = 'linear')(n_layer)\n    n_layer = BatchNormalization()(n_layer)\n    n_layer = Activation('relu')(n_layer)\n    n_layer = MaxPool2D((2,2))(n_layer)\nn_layer = Flatten()(n_layer)\nn_layer = Dense(32, activation = 'linear')(n_layer)\nn_layer = Dropout(0.5)(n_layer)\nn_layer = BatchNormalization()(n_layer)\nn_layer = Activation('relu')(n_layer)\nfeature_model = Model(inputs = [img_in], outputs = [n_layer], name = 'FeatureGenerationModel')\nfeature_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import concatenate\nimg_a_in = Input(shape = X_train.shape[1:], name = 'ImageA_Input')\nimg_b_in = Input(shape = X_train.shape[1:], name = 'ImageB_Input')\nimg_a_feat = feature_model(img_a_in)\nimg_b_feat = feature_model(img_b_in)\ncombined_features = concatenate([img_a_feat, img_b_feat], name = 'merge_features')\ncombined_features = Dense(16, activation = 'linear')(combined_features)\ncombined_features = BatchNormalization()(combined_features)\ncombined_features = Activation('relu')(combined_features)\ncombined_features = Dense(4, activation = 'linear')(combined_features)\ncombined_features = BatchNormalization()(combined_features)\ncombined_features = Activation('relu')(combined_features)\ncombined_features = Dense(1, activation = 'sigmoid')(combined_features)\nsimilarity_model = Model(inputs = [img_a_in, img_b_in], outputs = [combined_features], name = 'Similarity_Model')\nsimilarity_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity_model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['mae'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def siam_gen(in_groups, batch_size = 4):\n    while True:\n        pv_a, pv_b, pv_sim = gen_random_batch(train_groups, batch_size//2)\n        yield [pv_a, pv_b], pv_sim\n# we want a constant validation group to have a frame of reference for model performance\nvalid_a, valid_b, valid_sim = gen_random_batch(test_groups, 10)\nloss_history = similarity_model.fit_generator(siam_gen(train_groups), \n                               steps_per_epoch = 100,\n                               validation_data=([valid_a, valid_b], valid_sim),\n                                              epochs = 100,\n                                             verbose = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bag Of words as feature Extractor","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n# Split Data\nlb = LabelBinarizer()\nY = lb.fit_transform(df.label)\n\nX_train, X_test,y_train,y_test = train_test_split(df.sequences, Y, test_size = 0.2, random_state = 1)\n\n\n\n\n\n\ny_test_cat=keras.utils.to_categorical(y_test)\ny_train_cat=keras.utils.to_categorical(y_train)\n# Create a Count Vectorizer to gather the unique elements in sequence\nvect = CountVectorizer(analyzer = 'char_wb', ngram_range = (4,4))\n\n# Fit and Transform CountVectorizer\nvect.fit(X_train)\nX_train_df = vect.transform(X_train)\nX_test_df = vect.transform(X_test)\n\n#Print a few of the features\nprint(vect.get_feature_names()[-20:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Machine Learning Algorithm (MLA) Selection and Initialization\n\n\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n    XGBClassifier()    \n    ]\n\n\n\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy' ]\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n   # cv_results = model_selection.cross_validate(alg, X_train, y_train)\n    alg.fit(X_train_df.toarray(), y_train)\n    y_pred=alg.predict(X_test_df.toarray())\n    score=metrics.accuracy_score(y_test, y_pred)\n    \n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] =score\n\n    \n    \n    row_index+=1\n\n    \n\nMLA_compare\n#MLA_predict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion \n\nFrom this we can conclude that because of less number of samples we are not doing that much good by using deep learning.\nI will upload my next kernal in which I will do Feature engineering and try some machine learning algorithm , Optimization Methods and ensemble technique,","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}